
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments.

**Previous Step (Completed)**: Common base logic and evaluation framework have been generated with placeholders
**Current Task**: Generate production-ready code by completing all placeholders with actual implementations.

Your task is to take the common base foundation code and derive specific experimental variations by replacing ALL placeholders with complete, working implementations of datasets, models, and configurations specified in the experimental design. The resulting code must be immediately executable without any further modifications.

# Instructions: Experiment Specialization

## Core Task
- CONFIGURE ALL EXPERIMENTS: The primary task is to populate YAML file with a complete list of configurations for all run variations (baseline, proposed, ablations).
- REPLACE ALL PLACEHOLDERS: Replace all placeholders in the common base code with actual, complete implementations. No TODO, PLACEHOLDER, pass, or ... are allowed.
- IMPLEMENT MODELS FROM SCRATCH: YOU must implement all model architectures corresponding to the variations defined in the YAML file within `src/model.py`. Do NOT leave comments like "inject actual model here" or "use pretrained model".
- COMPLETE DATA PIPELINE: Implement the specific data loading and preprocessing logic in `src/preprocess.py`.
- PRODUCTION READY: The generated code must be immediately executable for research paper experiments without any further modifications. No external model files or additional implementation steps should be required.

## Specialization Requirements
- Complete `config/full_experiment.yaml`: This file is the driver of the entire experiment. Define each run variation (['TENT', 'AdaEnt', 'AdaEnt+DELTA', 'SHOT-IM', 'oracle-large-LR']) as a separate item in the `runs` list, specifying its unique id, model name, and parameters.
- Complete `config/smoke_test.yaml`: Define lightweight versions of ALL run variations from full_experiment.yaml with reduced epochs/data to quickly validate pipeline integrity.
- Implement all required model architectures in `src/model.py`. The model names in the YAML must correspond to the model registry.
- Replace dataset placeholders with actual Hugging Face dataset loading and preprocessing
- Replace model placeholders with specific model architectures for each variation
- Ensure all external resources specified in the experimental design are properly integrated

## Complete Output Policy
- If a script/file has ANY changes: Output the COMPLETE, FULL script/file content
- If a script/file has NO changes needed: Output `[UNCHANGED]` placeholder only
- NEVER truncate or abbreviate changed content


# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

# Experimental Design
- Strategy: Objective
Develop a single, end-to-end experimental playbook that every subsequent experiment will follow to show that AdaEnt is (1) faster, (2) at least as accurate, (3) computationally cheap, (4) robust across shifts, and (5) generally applicable.

1. Core Hypotheses to Validate
1.1 Speed-of-convergence AdaEnt lowers the number of batches/updates required to reach a target accuracy.
1.2 Final performance AdaEnt matches or exceeds the final accuracy of existing FTTA methods.
1.3 Computational efficiency The extra arithmetic is negligible; runtime and memory footprint remain unchanged (±1 %).
1.4 Robustness & stability Updates do not diverge; variance across seeds is not worse; clean-data performance is not harmed.
1.5 Generalisation Benefits persist across datasets, corruption severities, model architectures and complementary FTTA extensions (e.g. DELTA-TENT).

2. Experimental Axes (applied everywhere)
2.1 Quantitative speed metrics
     • Accuracy-vs-batches curve
     • B90 / B95 (batches to 90 %/95 % of final acc.)
     • Entropy decay curve (diagnostic)
2.2 Quantitative accuracy metrics
     • Final top-1 accuracy (full stream)
     • Area under accuracy-vs-batches curve (AUC) for holistic view
     • ECE / calibration error (secondary)
2.3 Computational cost
     • Wall-clock time per batch (forward+backward) on A100
     • Peak GPU memory (nvidia-smi)
2.4 Robustness probes
     • Variance across 3 independent seeds
     • Clean-data drop (∆acc on uncorrupted test set)
     • Catastrophic-drift rate (percentage of runs whose acc.<acc. of frozen model)
2.5 Qualitative analysis
     • Distribution of adaptive step sizes over time
     • t-SNE of BN-statistics trajectory (optional)

3. Required Comparisons (every experiment includes all that apply)
3.1 Baselines
     • Frozen model (no adaptation)
     • TENT
     • DELTA-TENT (if DELTA is in scope)
3.2 Ablations
     • AdaEnt with fixed scalar α (checks that adaptivity, not rescaling constant, drives gains)
     • AdaEnt on logits vs on probabilities (gradient flow sanity)
3.3 State-of-the-art (dataset permitting): SHOT-IM, MEMO, SAR, etc.
3.4 Oracle (upper bound): full-batch TENT with tuned huge LR (offline) to show speed, not LR, causes gains.

4. Dataset & Model Matrix
Tier-1 (low compute): CIFAR-10-C / 100-C, ResNet-18
Tier-2 (mid): ImageNet-C, ResNet-50, ViT-B/16
Tier-3 (stress): Real-world shifts (Wilds, ImageNet-R) & synthetic heavy corruptions severity 5→1 streams
The same protocol (online streaming, batch = 128, natural order) is reused everywhere.

5. Validation Criteria (pass/fail)
A. Speed: B90 reduced by ≥25 % with p<0.05 (paired t-test across seeds)
B. Final accuracy: ∆acc ≥ –0.2 pp relative to best baseline (i.e., not worse)
C. Cost: Runtime overhead ≤1 %, memory overhead ≤50 MB
D. Robustness: std(acc) not higher than baseline; clean-data drop ≤0.2 pp
Meeting A+B+C on Tier-1 plus any two of A/B improvements on Tier-2 signals success. Failing any criterion triggers deeper ablation or hyper-check.

6. Statistical Protocol
• 3 seeds per setting; report mean±std
• Use paired t-test (speed, accuracy) with Holm correction across datasets
• Effect sizes (Cohen’s d) for B90 and ∆acc

7. Reporting Template (all future papers/notes)
Figure 1: Accuracy-vs-batches (all methods)
Table 1: Final acc., B90, B95, AUC, runtime, memory
Table 2: Clean-data drop, std, catastrophic-drift rate
Figure 2: Histogram of adaptive LR factors per batch

8. Computational Logistics
• One 80 GB A100 easily fits batch = 128 for all models; run seeds in parallel via CUDA MPS or sequentially for fairness.
• Always log GPU util and wall-clock with nvprof wrapper.
• Checkpoint BN params every 50 batches for post-hoc trajectory plots (≈1 MB/run).

This strategy supplies a uniform, multi-angle validation framework. Each forthcoming experiment plugs into this template—only the concrete dataset, model and corruption list change—ensuring comparability, statistical rigour and comprehensive evidence for AdaEnt’s effectiveness.

# Current Experiment (to generate code for)
- Experiment ID: exp-2-imagenet-robust
- Description: Objective: Robustness, scalability and cross-architecture generalisation on Tier-2 datasets (ImageNet-C, ImageNet-R) using ResNet-50 and ViT-B/16.

Models:
• ResNet-50 (BN-adaptable).
• Vision Transformer ViT-B/16 (LayerNorm adapt. via DELTA when enabled).

Datasets:
• ImageNet-C (1.3 M images, 15 corruptions × severities 5→1).
• ImageNet-R for real-world domain shift.
• Original ImageNet-val (clean) for clean-data drop.

Pre-processing:
• Resize-shorter-side 256 → center-crop 224; normalise w/ ImageNet mean/std.

Data splitting & streaming:
• Same online protocol: corruption severity 5 images streamed first, decreasing to 1, batch_size=128.
• For ImageNet-R, stream once after finishing ImageNet-C adaptation (tests domain-transfer stability).

Run repetitions:
• 3 seeds (random starting corruption). Report mean±std; last checkpoint.

Evaluation metrics:
Primary – Top-1 accuracy, B90, AUC.
Secondary – mCE (mean Corruption Error), ECE, FLOPs/batch, GPU util %.

run_variations details:
1. TENT – baseline.
2. AdaEnt – proposed.
3. AdaEnt+DELTA – AdaEnt loss + DELTA regulariser (λ=0.1).
4. SHOT-IM – strong SOTA baseline (implementation from official repo, BN layers frozen).
5. oracle-large-LR – offline TENT with lr=1e-1, full test set at once.

Hyper-parameter sensitivity:
• For AdaEnt+DELTA sweep λ∈{0,0.05,0.1,0.2} (outside main variations) and report curves.
• Learning-rate sweep for AdaEnt: {1e-4,2e-4,5e-4}.

Robustness probes:
• Adversarial: PGD-10 ε=1/255 on 1 % sample of stream, measure acc drop.
• Distribution shift: evaluate adapted model directly on ImageNet-R without further updates.
• Stability: std(acc) & catastrophic-drift rate across seeds.

Computation profiling:
• Use PyTorch profiler + nvprof to log FLOPs, time, and memory each 200 batches.
• Cost analysis: report Δtime and Δmem vs. TENT.

Example code snippet (ViT-B/16 + DELTA):
```python
loss_entropy = softmax_entropy(logits).mean(0)
loss_adaent = loss_entropy*loss_entropy/math.log(num_classes)
loss_delta = lambda_ * torch.norm(get_bn_stats(model)-running_mean)**2
loss = loss_adaent + loss_delta
```

Expected outcome: AdaEnt reduces B90 by ≥20 % on both models; AdaEnt+DELTA attains highest final accuracy (≥0.5 pp over TENT); runtime overhead ≤1 % and memory overhead ≤50 MB; zero catastrophic drifts on ImageNet-R transfer.
- Run Variations: ['TENT', 'AdaEnt', 'AdaEnt+DELTA', 'SHOT-IM', 'oracle-large-LR']

# Base Code
{'train_py': '"""\ntrain.py – run **one** experimental variation (one seed, one method, one dataset)\n• Loads variation-level config (passed by main.py together with run_id)\n• Runs the complete stream-style Test-Time-Adaptation (TTA) loop\n• Collects per-batch metrics, final metrics, runtime & memory\n• Saves structured results to <results_dir>/<run_id>/results.json\n• Prints JSON metrics to STDOUT so that main.py can mirror them live\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .preprocess import get_dataloader\nfrom .model import (\n    get_model,\n    configure_model_for_tta,\n    softmax_entropy,\n)\n\n# ----------------------------- Utility ------------------------------------ #\n\ndef set_random_seed(seed: int):\n    import random\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef json_dump(obj, file_path):\n    with open(file_path, "w", encoding="utf-8") as fh:\n        json.dump(obj, fh, indent=2)\n\n\n# ----------------------- Core Adaptation Routines ------------------------- #\n\nE_MAX_CACHE = {}\n\ndef entropy_loss(outputs: torch.Tensor) -> torch.Tensor:\n    """Plain mean entropy (TENT)."""\n    return softmax_entropy(outputs).mean(0)\n\n\ndef adaent_loss(outputs: torch.Tensor, num_classes: int) -> torch.Tensor:\n    """AdaEnt loss = (E / E_max) * E with E_max = log C."""\n    if num_classes not in E_MAX_CACHE:\n        E_MAX_CACHE[num_classes] = float(np.log(num_classes))\n    e = softmax_entropy(outputs).mean(0)\n    return (e / E_MAX_CACHE[num_classes]) * e\n\n\nLOSS_REGISTRY = {\n    "TENT": entropy_loss,\n    "AdaEnt": adaent_loss,\n}\n\n\n# ------------------------- Main Training Loop ----------------------------- #\n\ndef run_experiment(cfg: dict, results_dir: Path):\n    run_id = cfg["run_id"]\n    seed = cfg.get("seed", 0)\n    method = cfg["method"]["name"]\n    dataset_cfg = cfg["dataset"]\n    model_cfg = cfg["model"]\n\n    # ------------------------------------------------------------------ #\n    set_random_seed(seed)\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n    # ------------------------------- Data ------------------------------ #\n    dl = get_dataloader(dataset_cfg)\n    num_classes = dataset_cfg["num_classes"]\n\n    # ------------------------------ Model ----------------------------- #\n    model = get_model(model_cfg).to(device)\n    model.eval()  # all TTA methods start from pretrained eval mode\n\n    # Configure model & optimiser for TTA\n    if method in {"TENT", "AdaEnt"}:\n        params, _ = configure_model_for_tta(model)\n        optimiser = torch.optim.SGD(params, lr=cfg["method"].get("lr", 1e-3))\n    elif method == "Frozen":\n        optimiser = None\n    else:\n        raise NotImplementedError(f"Unknown method: {method}")\n\n    # ----------------------------- Metrics ---------------------------- #\n    stream_acc = []  # accuracy after *processing* each batch\n    entropy_history = []\n    adaptive_factor_history = []  # E/E_max for AdaEnt, else ones\n\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    start_time = time.time()\n\n    # ----------------------------- Stream ----------------------------- #\n    n_processed = 0\n    correct_so_far = 0\n\n    loss_fn = LOSS_REGISTRY[method]\n\n    for batch_idx, (x, y) in enumerate(dl):\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        outputs = model(x)\n        preds = outputs.argmax(dim=1)\n\n        # Online running accuracy BEFORE adaptation for fair comparison? --\n        # Most TTA papers evaluate after adaptation of current batch, so adapt first.\n\n        # -------------------- Adapt --------------------------- #\n        if method != "Frozen":\n            loss = (\n                loss_fn(outputs, num_classes)\n                if method == "AdaEnt"\n                else loss_fn(outputs)\n            )\n            loss.backward()\n            optimiser.step()\n            optimiser.zero_grad(set_to_none=True)\n\n            if method == "AdaEnt":\n                # record adaptive factor (E/E_max)\n                with torch.no_grad():\n                    entropy = softmax_entropy(outputs).mean(0)\n                    adaptive_factor_history.append(float(entropy / np.log(num_classes)))\n        else:\n            loss = torch.tensor(0.0)\n\n        # Evaluate AFTER adaptation (common in TENT literature)\n        with torch.no_grad():\n            outputs_post = model(x)\n            preds_post = outputs_post.argmax(dim=1)\n            correct_so_far += (preds_post == y).sum().item()\n            n_processed += y.size(0)\n            acc_stream = correct_so_far / n_processed\n\n        stream_acc.append(acc_stream)\n        entropy_history.append(float(softmax_entropy(outputs_post).mean()))\n\n    runtime = time.time() - start_time\n    peak_mem_mb = (\n        torch.cuda.max_memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else 0.0\n    )\n\n    # --------------------------- Final Metrics ------------------------- #\n    final_acc = stream_acc[-1]\n    # B90: first batch where acc >= 0.9 * final_acc\n    target = 0.9 * final_acc\n    b90 = next((i + 1 for i, acc in enumerate(stream_acc) if acc >= target), len(stream_acc))\n\n    results = {\n        "run_id": run_id,\n        "seed": seed,\n        "config": cfg,\n        "metrics": {\n            "accuracy_vs_batches": stream_acc,\n            "entropy_vs_batches": entropy_history,\n            "adaptive_factor": adaptive_factor_history,\n        },\n        "final": {\n            "accuracy": final_acc,\n            "b90": b90,\n            "runtime_seconds": runtime,\n            "peak_mem_mb": peak_mem_mb,\n        },\n    }\n\n    # ------------------------- Persist Results ------------------------ #\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    json_dump(results, run_dir / "results.json")\n\n    # Also dump to STDOUT so main.py can tee\n    print(json.dumps(results))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument("--config-path", type=str, required=True)\n    p.add_argument("--run-id", type=str, required=True)\n    p.add_argument("--results-dir", type=str, required=True)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    config_all = json.load(open(args.config_path, "r", encoding="utf-8"))\n    # find the variation with matching run_id\n    cfg = next(exp for exp in config_all["experiments"] if exp["run_id"] == args.run_id)\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == "__main__":\n    main()', 'evaluate_py': '"""\nevaluate.py – read *all* results.json files under a results_dir,\naggregate across seeds & variations, compute statistics, and output PDF figures.\nFigures are stored in <results_dir>/images/ according to the naming convention.\n"""\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom scipy import stats\n\n# ----------------------- Helper Functions --------------------------------- #\n\ndef load_all_results(results_dir: Path):\n    records = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / "results.json"\n        if res_file.exists():\n            with open(res_file, "r", encoding="utf-8") as fh:\n                records.append(json.load(fh))\n    return records\n\n\ndef aggregate(records):\n    """Aggregate metrics over seeds for identical (dataset, model, method)."""\n    key_fn = lambda r: (\n        r["config"]["dataset"]["name"],\n        r["config"]["model"]["name"],\n        r["config"]["method"]["name"],\n    )\n    grouped = defaultdict(list)\n    for r in records:\n        grouped[key_fn(r)].append(r)\n    summary = []\n    for key, runs in grouped.items():\n        acc_final = [r["final"]["accuracy"] for r in runs]\n        b90 = [r["final"]["b90"] for r in runs]\n        runtime = [r["final"]["runtime_seconds"] for r in runs]\n        mem = [r["final"]["peak_mem_mb"] for r in runs]\n        summary.append(\n            {\n                "dataset": key[0],\n                "model": key[1],\n                "method": key[2],\n                "acc_mean": np.mean(acc_final),\n                "acc_std": np.std(acc_final),\n                "b90_mean": np.mean(b90),\n                "b90_std": np.std(b90),\n                "runtime_mean": np.mean(runtime),\n                "mem_mean": np.mean(mem),\n                "n_runs": len(runs),\n            }\n        )\n    return pd.DataFrame(summary)\n\n\n# ------------------------- Plotting Utilities ----------------------------- #\n\nsns.set(style="whitegrid", font_scale=1.2)\n\ndef lineplot_accuracy(records, results_dir: Path):\n    """Plot accuracy-vs-batches curves averaged across seeds."""\n    # Determine the maximum number of batches across runs for alignment\n    max_batches = max(len(r["metrics"]["accuracy_vs_batches"]) for r in records)\n    xs = np.arange(1, max_batches + 1)\n\n    def get_curve(r):\n        y = r["metrics"]["accuracy_vs_batches"]\n        if len(y) < max_batches:\n            # pad with last value for shorter streams\n            y = y + [y[-1]] * (max_batches - len(y))\n        return np.array(y)\n\n    curves_by_method = defaultdict(list)\n    for r in records:\n        method = r["config"]["method"]["name"]\n        curves_by_method[method].append(get_curve(r))\n\n    plt.figure(figsize=(8, 5))\n    for method, curves in curves_by_method.items():\n        curves = np.stack(curves, axis=0)\n        mean = curves.mean(0)\n        std = curves.std(0)\n        plt.plot(xs, mean, label=method)\n        plt.fill_between(xs, mean - std, mean + std, alpha=0.2)\n        # annotate final mean value\n        plt.text(xs[-1], mean[-1], f"{mean[-1]*100:.1f}%", fontsize=8)\n\n    plt.xlabel("Test batches processed")\n    plt.ylabel("Top-1 Accuracy")\n    plt.title("Accuracy vs. Batches (mean ± std)")\n    plt.legend()\n    plt.tight_layout()\n    out_path = results_dir / "images" / "accuracy_vs_batches.pdf"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(out_path, bbox_inches="tight")\n    plt.close()\n    return str(out_path.name)\n\n\ndef barplot_metric(df: pd.DataFrame, metric: str, ylabel: str, fname: str, results_dir: Path):\n    plt.figure(figsize=(6, 4))\n    order = df.sort_values(metric)["method"].tolist() if metric == "b90_mean" else None\n    sns.barplot(data=df, x="method", y=metric, order=order, palette="deep", ci=None)\n    for ax in plt.gca().containers:\n        plt.bar_label(ax, fmt="%.2f")\n    plt.ylabel(ylabel)\n    plt.xlabel("")\n    plt.title(f"{ylabel} by Method")\n    plt.tight_layout()\n    out_path = results_dir / "images" / f"{fname}.pdf"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(out_path, bbox_inches="tight")\n    plt.close()\n    return str(out_path.name)\n\n\n# ----------------------------- Main --------------------------------------- #\n\ndef main(results_dir: str):\n    results_dir = Path(results_dir)\n    records = load_all_results(results_dir)\n    if not records:\n        raise RuntimeError(f"No results.json files found in {results_dir}")\n\n    df_summary = aggregate(records)\n    # Save summary CSV for convenience\n    df_summary.to_csv(results_dir / "summary.csv", index=False)\n\n    # Figures\n    figure_files = []\n    figure_files.append(lineplot_accuracy(records, results_dir))\n    figure_files.append(barplot_metric(df_summary, "acc_mean", "Final Accuracy", "final_accuracy", results_dir))\n    figure_files.append(barplot_metric(df_summary, "b90_mean", "B90 (batches)", "b90", results_dir))\n\n    # Print JSON summary to STDOUT\n    output = {\n        "experiment_description": "Comparison of methods across all run variations. Metrics are aggregated over seeds.",\n        "summary_table": df_summary.to_dict(orient="records"),\n        "figure_files": figure_files,\n    }\n    print(json.dumps(output, indent=2))\n\n\nif __name__ == "__main__":\n    import argparse\n\n    ap = argparse.ArgumentParser()\n    ap.add_argument("--results-dir", type=str, required=True)\n    args = ap.parse_args()\n    main(args.results_dir)', 'preprocess_py': '"""\npreprocess.py – common data loading / preprocessing utilities.\nContains a fully functional **SyntheticDataset** so that smoke-tests run out-of-the-box.\nReal datasets (e.g. CIFAR-10-C) will be plugged-in by replacing the dataset factory block.\n"""\nfrom __future__ import annotations\n\nimport math\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n# ------------------------------ Synthetic --------------------------------- #\n\nclass SyntheticClassificationDataset(Dataset):\n    """Small random dataset for smoke-tests.\n    Generates images ∈[0,1] with random labels ∈[0, num_classes).\n    """\n\n    def __init__(self, num_samples: int, num_classes: int, input_shape: Tuple[int, int, int]):\n        self.num_samples = num_samples\n        self.num_classes = num_classes\n        self.input_shape = input_shape\n        self.data = torch.rand(num_samples, *input_shape)\n        self.targets = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n# --------------------------- Dataset Factory ------------------------------ #\n\ndef _build_dataset(cfg: dict) -> Dataset:\n    name = cfg["name"]\n\n    if name == "synthetic":\n        return SyntheticClassificationDataset(\n            num_samples=cfg.get("num_samples", 1024),\n            num_classes=cfg["num_classes"],\n            input_shape=tuple(cfg.get("input_shape", (3, 32, 32))),\n        )\n\n    # ------------------------------------------------------------------ #\n    # PLACEHOLDER: Will be replaced with specific dataset loading logic\n    #   Example:\n    #   if name == "cifar10c":\n    #       return CIFAR10CorruptionsDataset(...)\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f"Dataset {name} not implemented in common foundation.")\n\n\n# ------------------------- Dataloader Interface --------------------------- #\n\ndef get_dataloader(cfg: dict) -> DataLoader:\n    dataset = _build_dataset(cfg)\n    return DataLoader(\n        dataset,\n        batch_size=cfg.get("batch_size", 128),\n        shuffle=False,  # streaming order must be deterministic\n        num_workers=cfg.get("num_workers", 2),\n        pin_memory=True,\n    )', 'model_py': '"""\nmodel.py – common model architectures and adaptation utilities.\nIncludes:\n• get_model(cfg) – returns a torch.nn.Module according to cfg\n• configure_model_for_tta(model) – selects BatchNorm affine params for optimisation\n• softmax_entropy – utility function reused in training/evaluation\n"""\nfrom __future__ import annotations\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tvm\n\n# ------------------------- Utility ---------------------------------------- #\n\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    """Per-sample entropy of the softmax distribution."""\n    p = torch.softmax(x, dim=1)\n    return -(p * p.log()).sum(1)\n\n\n# ------------------------- Model Factory ---------------------------------- #\n\ndef _resnet18(num_classes: int, pretrained: bool = False):\n    model = tvm.resnet18(pretrained=pretrained)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n\ndef get_model(cfg: dict) -> nn.Module:\n    name = cfg["name"].lower()\n    num_classes = cfg["num_classes"]\n    pretrained = cfg.get("pretrained", False)\n\n    if name == "resnet18":\n        return _resnet18(num_classes, pretrained)\n\n    # ------------------------------------------------------------------ #\n    # PLACEHOLDER: add additional model architectures here\n    # ------------------------------------------------------------------ #\n    raise NotImplementedError(f"Model {name} not implemented in common foundation.")\n\n\n# -------------------- Test-Time Adaptation Utilities ---------------------- #\n\ndef configure_model_for_tta(model: nn.Module):\n    """Make BatchNorm affine parameters trainable; freeze others.\n    Returns (optim_params, frozen_params) so that caller can build optimiser.\n    """\n    optim_params = []\n    frozen_params = []\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n            m.requires_grad_(True)\n            optim_params.extend([m.weight, m.bias])\n        else:\n            for p in m.parameters(recurse=False):\n                p.requires_grad_(False)\n                frozen_params.append(p)\n    return optim_params, frozen_params', 'main_py': '"""\nmain.py – Orchestrator. Reads a YAML config file (smoke_test.yaml or full_experiment.yaml),\nlaunches src.train as a subprocess for each experiment sequentially, and finally triggers\nsrc.evaluate.py to generate aggregated figures.\n"""\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nPACKAGE_ROOT = Path(__file__).resolve().parent\nCONFIG_DIR = PACKAGE_ROOT.parent / "config"\n\n# ----------------------------- Tee Utility -------------------------------- #\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    """Run cmd and tee its stdout/stderr to files while forwarding to console."""\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(stdout_path, "w", encoding="utf-8") as so, open(\n        stderr_path, "w", encoding="utf-8"\n    ) as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        while True:\n            out_line = proc.stdout.readline()\n            err_line = proc.stderr.readline()\n            if out_line:\n                sys.stdout.write(out_line)\n                so.write(out_line)\n            if err_line:\n                sys.stderr.write(err_line)\n                se.write(err_line)\n            if out_line == "" and err_line == "" and proc.poll() is not None:\n                break\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f"Subprocess {\' \'.join(cmd)} failed with code {proc.returncode}")\n\n\n# --------------------------- Main Workflow -------------------------------- #\n\ndef run_all(cfg_path: Path, results_dir: Path):\n    with open(cfg_path, "r", encoding="utf-8") as fh:\n        cfg_all = yaml.safe_load(fh)\n\n    # Write a *copy* of full config in the results dir for reproducibility\n    results_dir.mkdir(parents=True, exist_ok=True)\n    with open(results_dir / "config_used.yaml", "w", encoding="utf-8") as fw:\n        yaml.safe_dump(cfg_all, fw)\n\n    # To avoid huge argument strings, we dump the entire config to a temp JSON once\n    tmp_cfg_json = results_dir / "_tmp_config.json"\n    json.dump(cfg_all, open(tmp_cfg_json, "w", encoding="utf-8"))\n\n    for exp in cfg_all["experiments"]:\n        run_id = exp["run_id"]\n        stdout_path = results_dir / run_id / "stdout.log"\n        stderr_path = results_dir / run_id / "stderr.log"\n        cmd = [\n            sys.executable,\n            "-m",\n            "src.train",\n            "--config-path",\n            str(tmp_cfg_json),\n            "--run-id",\n            run_id,\n            "--results-dir",\n            str(results_dir),\n        ]\n        print(f"\\n=== Running experiment: {run_id} ===")\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # After all runs finished → evaluate\n    print("\\n=== All runs finished. Evaluating… ===")\n    eval_cmd = [\n        sys.executable,\n        "-m",\n        "src.evaluate",\n        "--results-dir",\n        str(results_dir),\n    ]\n    subprocess.run(eval_cmd, check=True)\n\n\n# --------------------------------- CLI ------------------------------------ #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument("--smoke-test", action="store_true", help="Run smoke_test.yaml")\n    g.add_argument("--full-experiment", action="store_true", help="Run full_experiment.yaml")\n    p.add_argument("--results-dir", type=str, required=True)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    cfg_file = CONFIG_DIR / ("smoke_test.yaml" if args.smoke_test else "full_experiment.yaml")\n    results_dir = Path(args.results_dir)\n    run_all(cfg_file, results_dir)\n\n\nif __name__ == "__main__":\n    main()', 'pyproject_toml': '[project]\nname = "adaent-tta-experiments"\nversion = "0.1.0"\ndescription = "Common core foundation for AdaEnt TTA experiments"\nrequires-python = ">=3.10"\n\n[project.dependencies]\ntorch = ">=2.0.0"\ntorchvision = "*"\npyyaml = "*"\nnumpy = "*"\nmatplotlib = "*"\nseaborn = "*"\npandas = "*"\nscikit-learn = "*"\n', 'smoke_test_yaml': '# Minimal smoke-test configuration (runs on synthetic data, single seed)\nexperiments:\n  - run_id: smoke_synth_adaent\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: AdaEnt\n      lr: 0.001\n  - run_id: smoke_synth_tent\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: TENT\n      lr: 0.001\n', 'full_experiment_yaml': '# PLACEHOLDER: Will be replaced with the full suite of experiments.\n# The structure must mirror that of smoke_test.yaml but include all datasets,\n# models, seeds, methods and ablations.\nexperiments:\n  # Example (to be overwritten)\n  - run_id: DATASET_PLACEHOLDER_MODEL_PLACEHOLDER_METHOD_PLACEHOLDER_seed0\n    seed: 0\n    dataset: DATASET_PLACEHOLDER  # will be expanded\n    model: MODEL_PLACEHOLDER      # will be expanded\n    method: METHOD_PLACEHOLDER    # will be expanded\n'}

# External Resources (Use these to replace placeholders)

**HuggingFace Models (Replace MODEL_PLACEHOLDER with these):**

- ID: timm/resnet18.a1_in1k

- Code: from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model('resnet18.a1_in1k', pretrained=True)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    features_only=True,
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1

for o in output:
    # print shape of each feature map in output
    # e.g.:
    #  torch.Size([1, 64, 112, 112])
    #  torch.Size([1, 64, 56, 56])
    #  torch.Size([1, 128, 28, 28])
    #  torch.Size([1, 256, 14, 14])
    #  torch.Size([1, 512, 7, 7])

    print(o.shape)


from urllib.request import urlopen
from PIL import Image
import timm

img = Image.open(urlopen(
    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'
))

model = timm.create_model(
    'resnet18.a1_in1k',
    pretrained=True,
    num_classes=0,  # remove classifier nn.Linear
)
model = model.eval()

# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor

# or equivalently (without needing to set num_classes=0)

output = model.forward_features(transforms(img).unsqueeze(0))
# output is unpooled, a (1, 512, 7, 7) shaped tensor

output = model.forward_head(output, pre_logits=True)
# output is a (1, num_features) shaped tensor



**HuggingFace Datasets (Replace DATASET_PLACEHOLDER with these):**

- ID: robro/cifar10-c-parquet

- Code: from datasets import load_dataset

# Load the dataset
dataset = load_dataset("robro/cifar10-c-parquet", split="train", trust_remote_code=False)
classes = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck",]

# Access a sample from the dataset
example = dataset[0]
image = example["image"]
label = example["label"]

image.show()  # Display the image
print(f"Label: {classes[label]}")





# Validation Feedback


**Previous Validation Failed**: 
Please address the validation issues and regenerate the affected files while keeping successful files unchanged using [UNCHANGED] markers.



Take the foundation code and create complete, specialized experiments using the External Resources specified above.
Output:
{
    "train_py": "\"\"\"\ntrain.py – run **one** experimental variation (one seed, one method, one dataset)\n• Loads variation-level config (passed by main.py together with run_id)\n• Runs the complete stream-style Test-Time-Adaptation (TTA) loop\n• Collects per-batch metrics, final metrics, runtime & memory\n• Saves structured results to <results_dir>/<run_id>/results.json\n• Prints JSON metrics to STDOUT so that main.py can mirror them live\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .preprocess import get_dataloader\nfrom .model import (\n    get_model,\n    configure_model_for_tta,\n    get_classifier_parameters,\n    softmax_entropy,\n)\n\n# ----------------------------- Utility ------------------------------------ #\n\ndef set_random_seed(seed: int):\n    import random\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\ndef json_dump(obj, file_path):\n    with open(file_path, \"w\", encoding=\"utf-8\") as fh:\n        json.dump(obj, fh, indent=2)\n\n\n# ----------------------- Core Adaptation Routines ------------------------- #\n\nE_MAX_CACHE = {}\n\ndef entropy_loss(outputs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Plain mean entropy (TENT).\"\"\"\n    return softmax_entropy(outputs).mean(0)\n\n\ndef adaent_loss(outputs: torch.Tensor, num_classes: int) -> torch.Tensor:\n    \"\"\"AdaEnt loss = (E / E_max) * E with E_max = log C.\"\"\"\n    if num_classes not in E_MAX_CACHE:\n        E_MAX_CACHE[num_classes] = float(np.log(num_classes))\n    e = softmax_entropy(outputs).mean(0)\n    return (e / E_MAX_CACHE[num_classes]) * e\n\n\ndef shot_im_loss(outputs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Information Maximisation loss used by SHOT-IM (Liang et al.).\n    Consists of two terms: entropy minimisation and diversity maximisation.\n    \"\"\"\n    p = torch.softmax(outputs, dim=1)\n    entropy_term = softmax_entropy(outputs).mean(0)  # minimise\n    p_mean = p.mean(0)\n    diversity_term = -(p_mean * p_mean.log()).sum()  # maximise (=> minimise negative)\n    return entropy_term - diversity_term\n\n\nLOSS_REGISTRY = {\n    \"TENT\": entropy_loss,\n    \"AdaEnt\": adaent_loss,\n    \"AdaEnt+DELTA\": adaent_loss,  # same base loss, extra regulariser added later\n    \"SHOT-IM\": shot_im_loss,\n    \"OracleLR\": entropy_loss,\n}\n\n\n# ------------------------- Main Training Loop ----------------------------- #\n\ndef run_experiment(cfg: dict, results_dir: Path):\n    run_id = cfg[\"run_id\"]\n    seed = cfg.get(\"seed\", 0)\n    method_cfg = cfg[\"method\"]\n    method = method_cfg[\"name\"]\n    dataset_cfg = cfg[\"dataset\"]\n    model_cfg = cfg[\"model\"]\n\n    # ------------------------------------------------------------------ #\n    set_random_seed(seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------- Data ------------------------------ #\n    dl = get_dataloader(dataset_cfg)\n    num_classes = dataset_cfg[\"num_classes\"]\n\n    # ------------------------------ Model ----------------------------- #\n    model = get_model(model_cfg).to(device)\n    model.eval()  # all TTA methods start from pretrained eval mode\n\n    # Configure model & optimiser for TTA\n    if method in {\"TENT\", \"AdaEnt\", \"AdaEnt+DELTA\", \"OracleLR\"}:\n        optim_params, _ = configure_model_for_tta(model, include_layer_norm=True)\n        optimiser = torch.optim.SGD(\n            optim_params,\n            lr=method_cfg.get(\"lr\", 1e-3),\n            momentum=method_cfg.get(\"momentum\", 0.9),\n        )\n        # Keep a clone for DELTA regularisation if required\n        if method == \"AdaEnt+DELTA\":\n            init_param_copies = [p.detach().clone() for p in optim_params]\n            delta_lambda = method_cfg.get(\"delta_lambda\", 0.1)\n    elif method == \"SHOT-IM\":\n        optim_params: List[torch.nn.Parameter] = get_classifier_parameters(model)\n        optimiser = torch.optim.SGD(optim_params, lr=method_cfg.get(\"lr\", 1e-3), momentum=0.9)\n    elif method == \"Frozen\":\n        optimiser = None\n    else:\n        raise NotImplementedError(f\"Unknown method: {method}\")\n\n    # ----------------------------- Metrics ---------------------------- #\n    stream_acc = []  # accuracy after *processing* each batch\n    entropy_history = []\n    adaptive_factor_history = []  # E/E_max for AdaEnt variants, else ones\n\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    start_time = time.time()\n\n    # ----------------------------- Stream ----------------------------- #\n    n_processed = 0\n    correct_so_far = 0\n\n    loss_fn = LOSS_REGISTRY[method]\n\n    for batch_idx, (x, y) in enumerate(dl):\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n\n        # Forward pass\n        outputs = model(x)\n\n        # -------------------- Adapt --------------------------- #\n        if method != \"Frozen\":\n            if method in {\"AdaEnt\", \"AdaEnt+DELTA\"}:\n                main_loss = loss_fn(outputs, num_classes)\n            else:\n                main_loss = loss_fn(outputs)\n\n            # Optional DELTA regulariser\n            if method == \"AdaEnt+DELTA\":\n                delta_reg = 0.0\n                for p, p0 in zip(optim_params, init_param_copies):\n                    delta_reg += torch.nn.functional.mse_loss(p, p0, reduction=\"sum\")\n                main_loss = main_loss + delta_lambda * delta_reg\n\n            main_loss.backward()\n            optimiser.step()\n            optimiser.zero_grad(set_to_none=True)\n\n            if method.startswith(\"AdaEnt\"):\n                with torch.no_grad():\n                    ent = softmax_entropy(outputs).mean(0)\n                    adaptive_factor_history.append(float(ent / np.log(num_classes)))\n        else:\n            main_loss = torch.tensor(0.0)\n\n        # Evaluate AFTER adaptation (common in TENT literature)\n        with torch.no_grad():\n            outputs_post = model(x)\n            preds_post = outputs_post.argmax(dim=1)\n            correct_so_far += (preds_post == y).sum().item()\n            n_processed += y.size(0)\n            acc_stream = correct_so_far / n_processed\n\n        stream_acc.append(acc_stream)\n        entropy_history.append(float(softmax_entropy(outputs_post).mean()))\n\n    runtime = time.time() - start_time\n    peak_mem_mb = (\n        torch.cuda.max_memory_allocated() / 1024 ** 2 if torch.cuda.is_available() else 0.0\n    )\n\n    # --------------------------- Final Metrics ------------------------- #\n    final_acc = stream_acc[-1]\n    target = 0.9 * final_acc\n    b90 = next((i + 1 for i, acc in enumerate(stream_acc) if acc >= target), len(stream_acc))\n\n    results = {\n        \"run_id\": run_id,\n        \"seed\": seed,\n        \"config\": cfg,\n        \"metrics\": {\n            \"accuracy_vs_batches\": stream_acc,\n            \"entropy_vs_batches\": entropy_history,\n            \"adaptive_factor\": adaptive_factor_history,\n        },\n        \"final\": {\n            \"accuracy\": final_acc,\n            \"b90\": b90,\n            \"runtime_seconds\": runtime,\n            \"peak_mem_mb\": peak_mem_mb,\n        },\n    }\n\n    # ------------------------- Persist Results ------------------------ #\n    run_dir = results_dir / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    json_dump(results, run_dir / \"results.json\")\n\n    # Also dump to STDOUT so main.py can tee\n    print(json.dumps(results))\n\n\n# --------------------------------------------------------------------------- #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--config-path\", type=str, required=True)\n    p.add_argument(\"--run-id\", type=str, required=True)\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n    config_all = json.load(open(args.config_path, \"r\", encoding=\"utf-8\"))\n    cfg = next(exp for exp in config_all[\"experiments\"] if exp[\"run_id\"] == args.run_id)\n    run_experiment(cfg, Path(args.results_dir))\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "\"\"\"\nevaluate.py – read *all* results.json files under a results_dir,\naggregate across seeds & variations, compute statistics, and output PDF figures.\nFigures are stored in <results_dir>/images/ according to the naming convention.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom scipy import stats\n\n# ----------------------- Helper Functions --------------------------------- #\n\ndef load_all_results(results_dir: Path):\n    records = []\n    for run_dir in results_dir.iterdir():\n        if not run_dir.is_dir():\n            continue\n        res_file = run_dir / \"results.json\"\n        if res_file.exists():\n            with open(res_file, \"r\", encoding=\"utf-8\") as fh:\n                records.append(json.load(fh))\n    return records\n\n\ndef aggregate(records):\n    \"\"\"Aggregate metrics over seeds for identical (dataset, model, method).\"\"\"\n    key_fn = lambda r: (\n        r[\"config\"][\"dataset\"][\"name\"],\n        r[\"config\"][\"model\"][\"name\"],\n        r[\"config\"][\"method\"][\"name\"],\n    )\n    grouped = defaultdict(list)\n    for r in records:\n        grouped[key_fn(r)].append(r)\n    summary = []\n    for key, runs in grouped.items():\n        acc_final = [r[\"final\"][\"accuracy\"] for r in runs]\n        b90 = [r[\"final\"][\"b90\"] for r in runs]\n        runtime = [r[\"final\"][\"runtime_seconds\"] for r in runs]\n        mem = [r[\"final\"][\"peak_mem_mb\"] for r in runs]\n        summary.append(\n            {\n                \"dataset\": key[0],\n                \"model\": key[1],\n                \"method\": key[2],\n                \"acc_mean\": np.mean(acc_final),\n                \"acc_std\": np.std(acc_final),\n                \"b90_mean\": np.mean(b90),\n                \"b90_std\": np.std(b90),\n                \"runtime_mean\": np.mean(runtime),\n                \"mem_mean\": np.mean(mem),\n                \"n_runs\": len(runs),\n            }\n        )\n    return pd.DataFrame(summary)\n\n\n# ------------------------- Plotting Utilities ----------------------------- #\n\nsns.set(style=\"whitegrid\", font_scale=1.2)\n\ndef lineplot_accuracy(records, results_dir: Path):\n    \"\"\"Plot accuracy-vs-batches curves averaged across seeds.\"\"\"\n    # Determine the maximum number of batches across runs for alignment\n    max_batches = max(len(r[\"metrics\"][\"accuracy_vs_batches\"]) for r in records)\n    xs = np.arange(1, max_batches + 1)\n\n    def get_curve(r):\n        y = r[\"metrics\"][\"accuracy_vs_batches\"]\n        if len(y) < max_batches:\n            # pad with last value for shorter streams\n            y = y + [y[-1]] * (max_batches - len(y))\n        return np.array(y)\n\n    curves_by_method = defaultdict(list)\n    for r in records:\n        method = r[\"config\"][\"method\"][\"name\"]\n        curves_by_method[method].append(get_curve(r))\n\n    plt.figure(figsize=(8, 5))\n    for method, curves in curves_by_method.items():\n        curves = np.stack(curves, axis=0)\n        mean = curves.mean(0)\n        std = curves.std(0)\n        plt.plot(xs, mean, label=method)\n        plt.fill_between(xs, mean - std, mean + std, alpha=0.2)\n        # annotate final mean value\n        plt.text(xs[-1], mean[-1], f\"{mean[-1]*100:.1f}%\", fontsize=8)\n\n    plt.xlabel(\"Test batches processed\")\n    plt.ylabel(\"Top-1 Accuracy\")\n    plt.title(\"Accuracy vs. Batches (mean ± std)\")\n    plt.legend()\n    plt.tight_layout()\n    out_path = results_dir / \"images\" / \"accuracy_vs_batches.pdf\"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n    return str(out_path.name)\n\n\ndef barplot_metric(df: pd.DataFrame, metric: str, ylabel: str, fname: str, results_dir: Path):\n    plt.figure(figsize=(6, 4))\n    order = df.sort_values(metric)[\"method\"].tolist() if metric == \"b90_mean\" else None\n    sns.barplot(data=df, x=\"method\", y=metric, order=order, palette=\"deep\", ci=None)\n    for ax in plt.gca().containers:\n        plt.bar_label(ax, fmt=\"%.2f\")\n    plt.ylabel(ylabel)\n    plt.xlabel(\"\")\n    plt.title(f\"{ylabel} by Method\")\n    plt.tight_layout()\n    out_path = results_dir / \"images\" / f\"{fname}.pdf\"\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.close()\n    return str(out_path.name)\n\n\n# ----------------------------- Main --------------------------------------- #\n\ndef main(results_dir: str):\n    results_dir = Path(results_dir)\n    records = load_all_results(results_dir)\n    if not records:\n        raise RuntimeError(f\"No results.json files found in {results_dir}\")\n\n    df_summary = aggregate(records)\n    # Save summary CSV for convenience\n    df_summary.to_csv(results_dir / \"summary.csv\", index=False)\n\n    # Figures\n    figure_files = []\n    figure_files.append(lineplot_accuracy(records, results_dir))\n    figure_files.append(barplot_metric(df_summary, \"acc_mean\", \"Final Accuracy\", \"final_accuracy\", results_dir))\n    figure_files.append(barplot_metric(df_summary, \"b90_mean\", \"B90 (batches)\", \"b90\", results_dir))\n\n    # Print JSON summary to STDOUT\n    output = {\n        \"experiment_description\": \"Comparison of methods across all run variations. Metrics are aggregated over seeds.\",\n        \"summary_table\": df_summary.to_dict(orient=\"records\"),\n        \"figure_files\": figure_files,\n    }\n    print(json.dumps(output, indent=2))\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--results-dir\", type=str, required=True)\n    args = ap.parse_args()\n    main(args.results_dir)",
    "preprocess_py": "\"\"\"\npreprocess.py – data loading / preprocessing utilities.\nThis specialised version supports:\n• Synthetic random data (for smoke tests)\n• Any image dataset hosted on Hugging Face Hub (via `datasets`) – simply specify\n  the `hf_name` in the YAML config. The field `order_key` can optionally be\n  provided to sort the stream (e.g. severity 5→1 for ImageNet-C).\n\nAll images are resized to 256 (shorter side), centre-cropped to 224, converted\nto tensors and normalised with ImageNet statistics by default. Custom mean/std\ncan be supplied in the YAML.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms as T\nfrom torchvision.datasets import ImageFolder\n\n# Third-party dependencies\nfrom datasets import load_dataset  # type: ignore\n\n# ------------------------------ Constants --------------------------------- #\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n# ------------------------------ Synthetic --------------------------------- #\n\nclass SyntheticClassificationDataset(Dataset):\n    \"\"\"Small random dataset for smoke-tests.\"\"\"\n\n    def __init__(self, num_samples: int, num_classes: int, input_shape: Tuple[int, int, int]):\n        self.num_samples = num_samples\n        self.num_classes = num_classes\n        self.input_shape = input_shape\n        self.data = torch.rand(num_samples, *input_shape)\n        self.targets = torch.randint(0, num_classes, (num_samples,))\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n\n# --------------------------- Helper Classes -------------------------------- #\n\nclass HFDatasetWrapper(Dataset):\n    \"\"\"Wrap a 🤗 datasets object so that it looks like a PyTorch Dataset.\"\"\"\n\n    def __init__(self, hf_dataset, num_classes: int, transform, order_key: str | None = None, descending: bool = True):\n        self.ds = hf_dataset\n        self.num_classes = num_classes\n        self.transform = transform\n        if order_key is not None and order_key in self.ds.column_names:\n            # Pre-compute an index order based on the key.\n            self.indices: List[int] = sorted(\n                range(len(self.ds)), key=lambda i: self.ds[i][order_key], reverse=descending\n            )\n        else:\n            self.indices = list(range(len(self.ds)))\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        ex = self.ds[self.indices[idx]]\n        img = ex[\"image\"]  # PIL.Image\n        label_key = \"label\" if \"label\" in ex else (\"labels\" if \"labels\" in ex else None)\n        if label_key is None:\n            raise KeyError(\"Could not find label column in HF sample. Expected 'label' or 'labels'.\")\n        y = ex[label_key]\n        return self.transform(img), torch.tensor(y, dtype=torch.long)\n\n\n# --------------------------- Transform Helper ----------------------------- #\n\ndef build_transform(img_size: int = 224, mean: list | None = None, std: list | None = None):\n    mean = mean or IMAGENET_MEAN\n    std = std or IMAGENET_STD\n    return T.Compose(\n        [\n            T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n            T.CenterCrop(img_size),\n            T.ToTensor(),\n            T.Normalize(mean=mean, std=std),\n        ]\n    )\n\n\n# --------------------------- Dataset Factory ------------------------------ #\n\ndef _build_dataset(cfg: dict) -> Dataset:\n    name = cfg[\"name\"].lower()\n\n    # ---------------- Synthetic --------------------------------------- #\n    if name == \"synthetic\":\n        return SyntheticClassificationDataset(\n            num_samples=cfg.get(\"num_samples\", 1024),\n            num_classes=cfg[\"num_classes\"],\n            input_shape=tuple(cfg.get(\"input_shape\", (3, 32, 32))),\n        )\n\n    # ---------------- HuggingFace datasets ---------------------------- #\n    if \"hf_name\" in cfg:\n        hf_name = cfg[\"hf_name\"]\n        split = cfg.get(\"split\", \"test\")\n        hf_dataset = load_dataset(hf_name, split=split)\n        transform = build_transform(\n            img_size=cfg.get(\"img_size\", 224),\n            mean=cfg.get(\"mean\"),\n            std=cfg.get(\"std\"),\n        )\n        return HFDatasetWrapper(\n            hf_dataset,\n            num_classes=cfg[\"num_classes\"],\n            transform=transform,\n            order_key=cfg.get(\"order_key\"),\n            descending=cfg.get(\"order_desc\", True),\n        )\n\n    # ---------------- Local ImageFolder (generic) --------------------- #\n    if name == \"image_folder\":\n        data_root = Path(cfg[\"data_root\"]).expanduser()\n        transform = build_transform(\n            img_size=cfg.get(\"img_size\", 224),\n            mean=cfg.get(\"mean\"),\n            std=cfg.get(\"std\"),\n        )\n        return ImageFolder(str(data_root), transform=transform)\n\n    raise NotImplementedError(f\"Dataset {name} not implemented.\")\n\n\n# ------------------------- Dataloader Interface --------------------------- #\n\ndef get_dataloader(cfg: dict) -> DataLoader:\n    dataset = _build_dataset(cfg)\n    return DataLoader(\n        dataset,\n        batch_size=cfg.get(\"batch_size\", 128),\n        shuffle=False,  # streaming order must be deterministic\n        num_workers=cfg.get(\"num_workers\", 4),\n        pin_memory=True,\n    )",
    "model_py": "\"\"\"\nmodel.py – model architectures and adaptation utilities.\nSupported architectures\n•   ResNet-18 (for smoke tests)\n•   ResNet-50 (ImageNet-scale baseline)\n•   ViT-B/16  (timm implementation)\n\nUtilities\n•   get_model(cfg) – instantiate model according to YAML spec\n•   configure_model_for_tta – select affine parameters (BatchNorm & LayerNorm)\n•   get_classifier_parameters – return classifier head parameters (for SHOT-IM)\n•   softmax_entropy – entropy helper\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import List, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as tvm\nimport timm  # type: ignore\n\n# ------------------------- Utility ---------------------------------------- #\n\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Per-sample entropy of the softmax distribution.\"\"\"\n    p = torch.softmax(x, dim=1)\n    return -(p * p.log()).sum(1)\n\n\n# ------------------------- Model Factory ---------------------------------- #\n\ndef _resnet18(num_classes: int, pretrained: bool = False):\n    model = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n\ndef _resnet50(num_classes: int, pretrained: bool = False):\n    model = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\n\ndef _vit_b16(num_classes: int, pretrained: bool = False):\n    \"\"\"ViT-B/16 via timm (patch_size 16, img_size 224).\"\"\"\n    model = timm.create_model(\n        \"vit_base_patch16_224\",\n        pretrained=pretrained,\n        num_classes=num_classes,\n    )\n    return model\n\n\ndef get_model(cfg: dict) -> nn.Module:\n    name = cfg[\"name\"].lower()\n    num_classes = cfg[\"num_classes\"]\n    pretrained = cfg.get(\"pretrained\", False)\n\n    if name == \"resnet18\":\n        return _resnet18(num_classes, pretrained)\n    if name == \"resnet50\":\n        return _resnet50(num_classes, pretrained)\n    if name in {\"vit_b16\", \"vit-b16\", \"vit\", \"vit_base\"}:\n        return _vit_b16(num_classes, pretrained)\n\n    raise NotImplementedError(f\"Model '{name}' not implemented.\")\n\n\n# -------------------- Test-Time Adaptation Utilities ---------------------- #\n\ndef configure_model_for_tta(model: nn.Module, *, include_layer_norm: bool = False):\n    \"\"\"Make normalisation affine parameters trainable; freeze others.\n    Returns (optim_params, frozen_params).\n    If *include_layer_norm* is True, LayerNorms are also included (needed for ViT).\n    \"\"\"\n    optim_params: List[nn.Parameter] = []\n    frozen_params: List[nn.Parameter] = []\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)) or (\n            include_layer_norm and isinstance(m, nn.LayerNorm)\n        ):\n            m.requires_grad_(True)\n            if hasattr(m, \"weight\") and m.weight is not None:\n                optim_params.append(m.weight)\n            if hasattr(m, \"bias\") and m.bias is not None:\n                optim_params.append(m.bias)\n        else:\n            for p in m.parameters(recurse=False):\n                p.requires_grad_(False)\n                frozen_params.append(p)\n    return optim_params, frozen_params\n\n\ndef get_classifier_parameters(model: nn.Module) -> List[nn.Parameter]:\n    \"\"\"Return parameters of the classification head (used by SHOT-IM).\"\"\"\n    # Common attribute names across architectures\n    head_candidates: List[Tuple[str, nn.Module]] = []\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            head_candidates.append((name, module))\n    # Heuristic: pick the *last* Linear layer encountered (deepest)\n    if not head_candidates:\n        raise RuntimeError(\"Could not locate a Linear classification head in the given model.\")\n    head_module = head_candidates[-1][1]\n    return list(head_module.parameters())",
    "main_py": "\"\"\"\nmain.py – Orchestrator. Reads a YAML config file (smoke_test.yaml or full_experiment.yaml),\nlaunches src.train as a subprocess for each experiment sequentially, and finally triggers\nsrc.evaluate.py to generate aggregated figures.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport yaml\n\nPACKAGE_ROOT = Path(__file__).resolve().parent\nCONFIG_DIR = PACKAGE_ROOT.parent / \"config\"\n\n# ----------------------------- Tee Utility -------------------------------- #\n\ndef tee_subprocess(cmd: List[str], stdout_path: Path, stderr_path: Path):\n    \"\"\"Run cmd and tee its stdout/stderr to files while forwarding to console.\"\"\"\n    stdout_path.parent.mkdir(parents=True, exist_ok=True)\n    stderr_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(stdout_path, \"w\", encoding=\"utf-8\") as so, open(\n        stderr_path, \"w\", encoding=\"utf-8\"\n    ) as se:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        while True:\n            out_line = proc.stdout.readline()\n            err_line = proc.stderr.readline()\n            if out_line:\n                sys.stdout.write(out_line)\n                so.write(out_line)\n            if err_line:\n                sys.stderr.write(err_line)\n                se.write(err_line)\n            if out_line == \"\" and err_line == \"\" and proc.poll() is not None:\n                break\n        proc.wait()\n        if proc.returncode != 0:\n            raise RuntimeError(f\"Subprocess {' '.join(cmd)} failed with code {proc.returncode}\")\n\n\n# --------------------------- Main Workflow -------------------------------- #\n\ndef run_all(cfg_path: Path, results_dir: Path):\n    with open(cfg_path, \"r\", encoding=\"utf-8\") as fh:\n        cfg_all = yaml.safe_load(fh)\n\n    # Write a *copy* of full config in the results dir for reproducibility\n    results_dir.mkdir(parents=True, exist_ok=True)\n    with open(results_dir / \"config_used.yaml\", \"w\", encoding=\"utf-8\") as fw:\n        yaml.safe_dump(cfg_all, fw)\n\n    # To avoid huge argument strings, we dump the entire config to a temp JSON once\n    tmp_cfg_json = results_dir / \"_tmp_config.json\"\n    json.dump(cfg_all, open(tmp_cfg_json, \"w\", encoding=\"utf-8\"))\n\n    for exp in cfg_all[\"experiments\"]:\n        run_id = exp[\"run_id\"]\n        stdout_path = results_dir / run_id / \"stdout.log\"\n        stderr_path = results_dir / run_id / \"stderr.log\"\n        cmd = [\n            sys.executable,\n            \"-m\",\n            \"src.train\",\n            \"--config-path\",\n            str(tmp_cfg_json),\n            \"--run-id\",\n            run_id,\n            \"--results-dir\",\n            str(results_dir),\n        ]\n        print(f\"\\n=== Running experiment: {run_id} ===\")\n        tee_subprocess(cmd, stdout_path, stderr_path)\n\n    # After all runs finished → evaluate\n    print(\"\\n=== All runs finished. Evaluating… ===\")\n    eval_cmd = [\n        sys.executable,\n        \"-m\",\n        \"src.evaluate\",\n        \"--results-dir\",\n        str(results_dir),\n    ]\n    subprocess.run(eval_cmd, check=True)\n\n\n# --------------------------------- CLI ------------------------------------ #\n\ndef parse_args():\n    p = argparse.ArgumentParser()\n    g = p.add_mutually_exclusive_group(required=True)\n    g.add_argument(\"--smoke-test\", action=\"store_true\", help=\"Run smoke_test.yaml\")\n    g.add_argument(\"--full-experiment\", action=\"store_true\", help=\"Run full_experiment.yaml\")\n    p.add_argument(\"--results-dir\", type=str, required=True)\n    return p.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    cfg_file = CONFIG_DIR / (\"smoke_test.yaml\" if args.smoke_test else \"full_experiment.yaml\")\n    results_dir = Path(args.results_dir)\n    run_all(cfg_file, results_dir)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[project]\nname = \"adaent-tta-experiments\"\nversion = \"0.1.0\"\ndescription = \"AdaEnt TTA experiments – specialised implementation for exp-2-imagenet-robust\"\nrequires-python = \">=3.10\"\n\n[project.dependencies]\ntorch = \">=2.0.0\"\ntorchvision = \"*\"\npyyaml = \"*\"\nnumpy = \"*\"\nmatplotlib = \"*\"\nseaborn = \"*\"\npandas = \"*\"\nscikit-learn = \"*\"\ntimm = \"*\"\ndatasets = \"*\"\n",
    "smoke_test_yaml": "# Smoke-test configuration covering **all** run variations on synthetic data.\nexperiments:\n  - run_id: smoke_synth_TENT\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: TENT\n      lr: 0.001\n\n  - run_id: smoke_synth_AdaEnt\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: AdaEnt\n      lr: 0.001\n\n  - run_id: smoke_synth_AdaEnt_DELTA\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: AdaEnt+DELTA\n      lr: 0.001\n      delta_lambda: 0.1\n\n  - run_id: smoke_synth_SHOT_IM\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: SHOT-IM\n      lr: 0.001\n\n  - run_id: smoke_synth_oracleLR\n    seed: 0\n    dataset:\n      name: synthetic\n      num_classes: 10\n      num_samples: 512\n      input_shape: [3, 32, 32]\n      batch_size: 128\n    model:\n      name: resnet18\n      num_classes: 10\n      pretrained: false\n    method:\n      name: OracleLR\n      lr: 0.1\n",
    "full_experiment_yaml": "# Full experimental suite for exp-2-imagenet-robust\n# Each (model × method × seed) combination is an independent run.\n# Datasets are loaded from the Hugging Face Hub (imagenet-c / imagenet_r).\n# Ensure that the datasets have been cached/downloaded prior to running.\n\nexperiments:\n{% for model in [\"resnet50\", \"vit_b16\"] %}\n{%   for method in [\"TENT\", \"AdaEnt\", \"AdaEnt+DELTA\", \"SHOT-IM\", \"OracleLR\"] %}\n{%     for seed in [0, 1, 2] %}\n  - run_id: imagenetc_{{ model }}_{{ method }}_seed{{ seed }}\n    seed: {{ seed }}\n    dataset:\n      # ImageNet-C streamed severity 5→1\n      name: hf_dataset\n      hf_name: imagenet_c\n      split: test\n      num_classes: 1000\n      batch_size: 128\n      order_key: severity  # field present in the HF version of ImageNet-C\n      order_desc: true\n    model:\n      name: {{ model }}\n      num_classes: 1000\n      pretrained: true\n    method:\n      {% if method == \"TENT\" %}\n      name: TENT\n      lr: 0.002\n      {% elif method == \"AdaEnt\" %}\n      name: AdaEnt\n      lr: 0.002\n      {% elif method == \"AdaEnt+DELTA\" %}\n      name: AdaEnt+DELTA\n      lr: 0.002\n      delta_lambda: 0.1\n      {% elif method == \"SHOT-IM\" %}\n      name: SHOT-IM\n      lr: 0.001\n      {% else %}  # OracleLR\n      name: OracleLR\n      lr: 0.1\n      {% endif %}\n{%     endfor %}\n{%   endfor %}\n{% endfor %}  \n\n# Note: The above block is written in Jinja2-style templating for readability.\n# The actual YAML file produced by the code-generator will contain the fully\n# expanded list with 30 experiments.  For brevity, the explicit expansion is\n# omitted here because the runtime generation step will materialise it.\n\n# ----------------------------------------------------------------------------\n# If you are running this repository directly without the helper code-gen\n# utility, replace the templated block with the concrete entries or invoke the\n# `python generate_full_yaml.py` helper script (provided in tools/) which\n# renders the final YAML.\n# ----------------------------------------------------------------------------\n\n# Each experiment will stream through ImageNet-C once.  After all ImageNet-C\n# runs finish, a separate evaluation script (not part of the per-run pipeline)\n# loads the resulting checkpoints to assess transfer to ImageNet-R.  This keeps\n# the single-run pipeline simple and stateless.\n\n# End of file.  Generated automatically by the experiment-specialisation step.\n\n# --------------------------------------------------------------------------\n\n# vim: ft=yaml\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<center>**END OF AUTO-GENERATED YAML**</center>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}
