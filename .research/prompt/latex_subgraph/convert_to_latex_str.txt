
Input:

You are a LaTeX expert.
Your task is to convert each section of a research paper into plain LaTeX **content only**, without including any section titles or metadata.

Below are the paper sections. For each one, convert only the **content** into LaTeX:

---
Section: title

AdaEnt: Error-Proportional Entropy Minimisation for Fast Test-Time Adaptation

---

---
Section: abstract

Fully test-time adaptation (FTTA) seeks to update a pre-trained classifier online, without labels, when the test distribution shifts. Popular approaches such as TENT minimise the mean softmax entropy of the current batch while adapting only batch-normalisation (BN) affine parameters. To avoid catastrophic drift they employ a small, fixed learning rate, which makes early optimisation sluggish even though the model is furthest from optimality in the first few batches. We introduce AdaEnt, a one-line, parameter-free replacement of the loss. Instead of the plain entropy E we minimise L = (E ⁄ E_max) · E, where E_max = log C for C classes. The gradient is therefore multiplied by E ⁄ E_max: large when predictions are uncertain, negligible when the model is confident. AdaEnt preserves all other aspects of TENT—adapted parameters, optimiser, and code structure—yet realises an error-proportional step size akin to classical Polyak rules. In a unified online-stream protocol on CIFAR-10-C, CIFAR-100-C, ImageNet-C and ImageNet-R, AdaEnt cuts the batches-to-90-percent-of-final-accuracy by roughly 40 % on CIFAR-C and 20 % on ImageNet-C⁄R, matches or slightly improves final accuracy, adds under 1 % overhead in time and memory, and maintains stability. Combined with DELTA it further boosts accuracy, showing complementarity. The evidence demonstrates that AdaEnt is a robust, computationally free upgrade for entropy-based FTTA.

---

---
Section: introduction

Distribution shift at deployment remains a major obstacle for reliable machine-learning systems. Fully test-time adaptation (FTTA) tackles a particularly demanding scenario: a pre-trained classifier meets an unlabeled, possibly correlated stream of test samples and must adapt online, in real time, using only those samples. Entropy minimisation is a simple yet widely adopted recipe for FTTA. By updating only the affine parameters of batch normalisation (BN) layers, methods such as TENT deliver respectable accuracy while keeping computational cost low and avoiding catastrophic drift. Nevertheless, a limiting factor persists. The learning rate is fixed and therefore does not reflect how far the current batch is from a confident prediction. Early in the stream the mean entropy is high, signalling large error, but the update remains small; later, when entropy is low, the step size is still the same, risking over-correction. This step-size mismatch slows convergence and delays the moment when the system reaches useful accuracy, a serious drawback in safety-critical or interactive settings where every early mistake matters.  We address this problem with AdaEnt, a minimal yet principled modification to the entropy loss. Let E be the mean softmax entropy of the current batch and let E_max = log C denote the maximal entropy for a C-class task. Standard TENT minimises L = E. AdaEnt instead minimises L = (E ⁄ E_max)·E, equivalently E^2 ⁄ E_max. The gradient is rescaled by E ⁄ E_max, which is close to one when predictions are almost uniform and approaches zero as confidence builds. Consequently, updates are aggressive exactly when the model is most uncertain and fade away as the residual error shrinks. No new hyper-parameters are introduced; the change is literally a single line of code.  The apparent simplicity belies a substantial empirical impact. Using a rigorously standardised experimental playbook we observe: (i) convergence speed improves markedly—about 40 % fewer batches are required to reach 90 % of final accuracy on CIFAR-C and about 20 % on ImageNet-C⁄R; (ii) final top-1 accuracy is matched or slightly exceeded; (iii) runtime and memory overheads remain below one percent; (iv) robustness indicators such as variance across seeds, clean-data accuracy, and catastrophic-drift rate are unchanged or better. Because AdaEnt modifies only the loss magnitude, it is compatible with orthogonal FTTA improvements that address statistics estimation and class bias, such as DELTA \cite{zhao-2023-delta}. Indeed, combining the two yields further accuracy gains.  Contributions • We diagnose the fixed-step limitation of entropy-based FTTA and formalise it as a step-size mismatch. • We propose AdaEnt, an error-proportional rescaling of the entropy loss that introduces no hyper-parameters and is implemented in one line. • We develop and release a unified evaluation protocol emphasising convergence speed, accuracy, computational cost and robustness. • Through extensive experiments on CIFAR-10-C, CIFAR-100-C, ImageNet-C and ImageNet-R we demonstrate that AdaEnt accelerates adaptation, preserves or improves accuracy, incurs negligible overhead, and composes seamlessly with DELTA.  Future work will explore self-scaled objectives beyond entropy, theoretical convergence guarantees under non-stationary streams, and integration with memory-based FTTA methods.

---

---
Section: related_work

Entropy-minimisation FTTA. TENT exemplifies the paradigm of adapting BN affine parameters by minimising batch entropy with a fixed learning rate. Its strengths are code simplicity and hardware efficiency, but slow early progress is an acknowledged drawback. AdaEnt retains the same objective direction while modulating its magnitude, thereby attacking the convergence-speed issue without altering any other component.  DELTA. Zhao et al. identify unreliable BN statistics and class-biased updates as two distinct weaknesses of existing FTTA pipelines and propose Test-time Batch Renormalisation plus Dynamic Online re-weighting to remedy them \cite{zhao-2023-delta}. These techniques are largely orthogonal to the step-size problem and can be applied together with AdaEnt. In our experiments the combination (AdaEnt+DELTA) achieves both faster convergence and higher final accuracy, underscoring complementarity.  Regularised test-time losses. In weakly supervised salient-object detection, Test-Time Adaptation with Regularised Loss demonstrates that carefully crafted objectives can improve adaptation \cite{author-year-test}. Such works motivate our focus on loss design, yet they introduce additional terms and hyper-parameters tailored to a task, whereas AdaEnt remains task-agnostic and parameter-free.  Alternative FTTA variants integrate memory buffers, confidence thresholds or self-training. While often improving final accuracy, they increase computational burden and hyper-parameter tuning effort. AdaEnt, in contrast, offers a drop-in speed boost at virtually zero cost and with no tuning.  Overall, prior art has tackled statistics estimation, class imbalance and task-specific regularisation; the present work is, to our knowledge, the first to solve the step-size mismatch in entropy-based FTTA through a self-scaled loss, yielding substantial practical benefits.

---

---
Section: background

Consider a classifier f_θ that maps an input x to logits z and softmax probabilities p. When the data distribution shifts, the original parameters θ may underperform. Fully test-time adaptation seeks to update θ online without labels. For stability and efficiency, the community typically restricts learning to the affine parameters γ and β of Batch Normalisation (BN) layers while freezing all other weights.  Entropy-minimisation objective. For a minibatch B the mean softmax entropy is E = (1 ⁄ |B|) Σ_i Σ_c −p_ic log p_ic. Its maximum, achieved by a uniform prediction, is E_max = log C where C is the number of classes. TENT performs gradient descent on γ and β to minimise E using a small, fixed learning rate. Because the learning rate is independent of E, early updates—even though the model is far from optimal—are tiny, and late updates can be disproportionately large relative to the remaining error.  Complementary advances. DELTA mitigates unreliable BN statistics via Test-time Batch Renormalisation and reduces class-biased updates through Dynamic Online re-weighting \cite{zhao-2023-delta}. These advances leave the step-size issue untouched and are therefore compatible with AdaEnt.  Problem setting adopted in this paper. Test examples stream in their natural order, processed in batches of 128. After each batch we compute the loss, update BN affine parameters, and discard the data. We monitor (i) accuracy over time, (ii) B90/B95—the number of batches required to reach 90 %/95 % of final accuracy, (iii) final top-1 accuracy, (iv) expected calibration error, (v) runtime and memory, and (vi) robustness indicators such as variance across seeds and catastrophic-drift events. This online regime reflects real deployment constraints and discourages oracle-like tuning that would require future data.

---

---
Section: method

AdaEnt alters only the loss magnitude used during FTTA. Given the mean entropy E of the current batch and E_max = log C, standard TENT minimises L_TENT = E. AdaEnt replaces it with  L_AdaEnt = (E ⁄ E_max) · E = E^2 ⁄ E_max.  The gradient with respect to the BN parameters is therefore scaled by the factor E ⁄ E_max, which lies in the interval . At high uncertainty (E≈E_max) the factor is near one, recovering the original gradient magnitude and allowing large corrective steps. As the classifier becomes confident (E→0) the factor shrinks, damping the updates automatically. This behaviour mirrors error-proportional step-size rules in classical optimisation, yet requires neither explicit per-iteration step computation nor any additional hyper-parameter.  Implementation is trivial: replace the entropy loss computation with loss = (entropy / E_max) * entropy and keep everything else—parameters to adapt, optimiser, momentum, weight decay—unchanged. Because the rescaling is bounded and monotonic, it cannot produce steps larger than those generated by the baseline when E ≤ E_max, ensuring stability.  Compatibility. AdaEnt leaves the gradient direction intact; any method that relies on entropy gradients (e.g. DELTA) can substitute AdaEnt’s self-scaled loss without modification. The idea is architecture-agnostic and applies equally to ResNets and Vision Transformers. While we focus on classification, nothing in the derivation is tied to this task; any unsupervised loss with a natural upper bound could be self-scaled in the same fashion.

---

---
Section: experimental_setup

Datasets and architectures. Tier-1 experiments use CIFAR-10-C and CIFAR-100-C with a ResNet-18 pre-trained on clean data. Corruptions are streamed in the order provided by the dataset, severities 5→1. Tier-2 employs ImageNet-C and ImageNet-R with two backbones: ResNet-50 and ViT-B⁄16. All runs process the stream in batches of 128 and adapt only BN affine parameters.  Methods compared. We evaluate the frozen (non-adapted) model, TENT, AdaEnt, DELTA-TENT, and AdaEnt combined with DELTA. Ablations include (i) AdaEnt-fixed-α, which substitutes the adaptive factor E ⁄ E_max with a constant 0.5, and (ii) a variant computing entropy on logits rather than probabilities to check gradient flow. Where available we also report strong FTTA baselines such as SHOT-IM.  Metrics. Primary speed metrics are the accuracy-versus-batches curve, B90, and B95 (lower is better). Accuracy metrics comprise final top-1 accuracy and the area under the accuracy curve (AUC, higher is better). Computational metrics are wall-clock time per batch (forward + backward) and peak GPU memory. Robustness metrics cover expected calibration error, variance across three seeds, clean-data drop, and catastrophic-drift rate (runs whose accuracy falls below that of the frozen model).  Statistical protocol. Each setting is run with three random seeds. We report mean ± standard deviation, use paired t-tests for speed and accuracy comparisons, apply Holm correction across datasets, and compute Cohen’s d for effect sizes.  Implementation details. All methods inherit the official TENT codebase. For AdaEnt we set E_max = log C once at initialisation and insert the self-scaled loss in place of the original entropy. DELTA variants follow the authors’ public implementation, with the same substitution of the loss function.  Hardware. Experiments run on a single NVIDIA A100 with 80 GB memory. Runtime and memory statistics are recorded via standard profiling tools; BN parameters are checkpointed every 50 batches for post-hoc analysis.

---

---
Section: results

Convergence speed. On CIFAR-10-C, AdaEnt reaches 90 % of its own final accuracy after 7.4 ± 0.4 batches, compared with 12.3 ± 0.6 for TENT—a 39.8 % reduction (p = 3.1×10⁻⁴, Cohen’s d = 3.1). CIFAR-100-C shows a similar 39.6 % cut (9.0 ± 0.3 vs 14.9 ± 0.5, p = 4.6×10⁻⁴). On ImageNet-C, ResNet-50 improves from 20.5 ± 0.7 to 16.2 ± 0.5 batches (-21.0 %), while ViT-B⁄16 improves from 24.1 ± 0.9 to 18.9 ± 0.6 (-21.6 %), both significant at p<0.01.  Early-batch accuracy. After a single adaptation step AdaEnt surpasses TENT by 2.4 percentage points on CIFAR-10-C and 1.9 on CIFAR-100-C. Gains of roughly 1.5 points are observed on ImageNet-C, confirming that the larger initial updates translate into immediate benefits.  Final top-1 accuracy. AdaEnt matches or slightly exceeds TENT across all settings: 68.0 ± 0.3 % vs 67.8 ± 0.2 % on CIFAR-10-C, 46.1 ± 0.3 % vs 46.0 ± 0.3 % on CIFAR-100-C, 43.4 ± 0.3 % vs 43.2 ± 0.3 % on ImageNet-C (ResNet-50), and 46.3 ± 0.4 % vs 46.1 ± 0.4 % on ImageNet-C (ViT-B⁄16). Combining AdaEnt with DELTA further lifts ImageNet-C accuracy to 44.0 % and 47.1 % respectively, outperforming the strong SHOT-IM baseline at 42.8 %.  Computational cost. The average time per batch is 28.5 ms for AdaEnt versus 28.4 ms for TENT (+0.3 %). Peak memory usage increases by only 38 MB (+0.6 %). Both numbers fall well below the 1 % overhead budget.  Robustness. Clean-data drops are −0.49 pp (AdaEnt) and −0.52 pp (TENT) on CIFAR-10. Catastrophic-drift events are absent in all AdaEnt runs but occur twice with TENT on ImageNet-C. Seed-to-seed variance is never higher than the baseline. Under a PGD-10 attack with ε = 1⁄255 the accuracies are nearly identical (17.9 % vs 17.8 %). Adapting on ImageNet-C and evaluating on ImageNet-R yields 37.6 % for AdaEnt, marginally above TENT’s 37.4 % and below AdaEnt+DELTA’s 38.2 %.  Ablations. Replacing the adaptive factor by a constant α=0.5 reduces B90 to 10.8, faster than TENT but notably slower than full AdaEnt; occasional overshoot is observed. Computing entropy on logits does not improve over the standard probability-based variant, validating the default design.  Oracle comparison. An offline TENT oracle with a tuned large learning rate achieves 68.1 % final accuracy on CIFAR-10-C but cannot be run online—it requires the entire test set at once and diverges if applied per batch. AdaEnt attains comparable accuracy under true online constraints and without learning-rate tuning.  Statistical summary. Across 30 paired measurements AdaEnt significantly improves convergence speed in 28, AUC in all 30, and final accuracy in 7 cases after Holm correction. Effect sizes for B90 are large (d>2 on CIFAR, ≈1.2 on ImageNet).  In short, the empirical evidence corroborates the theoretical intuition: self-scaled entropy delivers larger steps when they matter and smaller ones when they do not, yielding faster, stable, and cost-free adaptation.

---

---
Section: conclusion

AdaEnt tackles a fundamental inefficiency in entropy-based fully test-time adaptation: the mismatch between a fixed learning rate and the varying distance to the optimum. By multiplying the entropy gradient by the relative entropy E ⁄ E_max, AdaEnt realises an error-proportional step size without introducing any new hyper-parameters or altering the optimisation loop.  Extensive experiments on CIFAR-C and ImageNet-C⁄R confirm that this one-line change reduces convergence time by up to 40 %, maintains or slightly improves final accuracy, and leaves runtime and memory essentially unchanged. Robustness metrics remain intact, and integration with DELTA brings further gains, highlighting that AdaEnt complements rather than replaces existing FTTA advances \cite{zhao-2023-delta}.  The success of AdaEnt suggests a broader principle: self-scaling unsupervised objectives can provide safe, automatic adaptation dynamics in the absence of labels. Future research may extend the idea to other losses, derive theoretical guarantees under non-stationary streams, and explore adaptive mixtures of self-scaled objectives with regularisers such as those proposed in DELTA. Beyond classification, self-scaling may benefit tasks like segmentation or detection, building on evidence that careful test-time loss design is broadly effective \cite{author-year-test}.  In practice, AdaEnt offers practitioners a free, robust acceleration for FTTA: change one line, adapt faster.

---


## LaTeX Formatting Rules:
- Use \subsection{...} for any subsections within this section.
    - Subsection titles should be distinct from the section name;
    - Do not use '\subsection{  }', or other slight variations. Use more descriptive and unique titles.
    - Avoid excessive subdivision. If a subsection is brief or overlaps significantly with another, consider merging them for clarity and flow.

- For listing contributions, use the LaTeX \begin{itemize}...\end{itemize} format.
    - Each item should start with a short title in \textbf{...} format.
    - Avoid using -, *, or other Markdown bullet styles.

- When including tables, use the `tabularx` environment with `\textwidth` as the target width.
    - At least one column must use the `X` type to enable automatic width adjustment and line breaking.
    - Include `\hline` at the top, after the header, and at the bottom. Avoid vertical lines unless necessary.
    - To left-align content in `X` columns, define `
ewcolumntype{Y}{>{
aggedrightrraybackslash}X}` using the `array` package.

- When writing pseudocode, use the `algorithm` and `algorithmicx` LaTeX environments.
    - Only include pseudocode in the `Method` section. Pseudocode is not allowed in any other sections.
    - Prefer the `\begin{algorithmic}` environment using **lowercase commands** such as `\State`, `\For`, and `\If`, to ensure compatibility and clean formatting.
    - Pseudocode must represent actual algorithms or procedures with clear logic. Do not use pseudocode to simply rephrase narrative descriptions or repeat what has already been explained in text.
        - Good Example:
        ```latex
        \State Compute transformed tokens: \(	ilde{T} \leftarrow W\,T\)
        \State Update: \(T_{new} \leftarrow 	ilde{T} + \mu\,T_{prev}\)
        ```
- Figures and images are ONLY allowed in the "Results" section.
    - Use LaTeX float option `[H]` to force placement.

- All figures must be inserted using the following LaTeX format, using a `width` that reflects the filename:
    ```latex
    \includegraphics[width=\linewidth]{ images/filename.pdf }
    ```
    The `<appropriate-width>` must be selected based on the filename suffix:
    - If the filename ends with _pair1.pdf or _pair2.pdf, use 0.48\linewidth as the width of each subfigure environment and place the figures side by side using `subcaption` package.
    - Otherwise (default), use 0.7\linewidth

- **Escaping special characters**:
    - LaTeX special characters (`#`, `$`, `%`, `&`, `~`, `_`, `^`, `{`, `}`, `\`) must be escaped with a leading backslash when they appear in plain text (e.g., `data\_set`, `C\&C`).
    - Underscores **must always be escaped** (`\_`) outside math mode, even in filenames (e.g., memory\_profiler), code-style words, itemize lists, or citation contexts.

- Always use ASCII hyphens (`-`) instead of en-dashes (`–`) or em-dashes (`—`) to avoid spacing issues in hyphenated terms.
- Do not include any of these higher-level commands such as \documentclass{...}, \begin{document}, and \end{document}.
    - Additionally, avoid including section-specific commands such as \begin{abstract}, \section{  }, or any other similar environment definitions.
- Do not modify citation placeholders:
    - Citation placeholders appear in the format [citation_key], where citation_key contains underscores, numbers, and text (e.g., [vaswani_2017_attention], [smith_2023_deep]).
    - You must preserve these placeholders EXACTLY as they appear in the input text.
- If no LaTeX conversion is needed, output the content as-is without status messages like [Unchanged].
Output:
{
    "title": "AdaEnt: Error-Proportional Entropy Minimisation for Fast Test-Time Adaptation",
    "abstract": "Fully test-time adaptation (FTTA) seeks to update a pre-trained classifier online, without labels, when the test distribution shifts. Popular approaches such as TENT minimise the mean softmax entropy of the current batch while adapting only batch-normalisation (BN) affine parameters. To avoid catastrophic drift they employ a small, fixed learning rate, which makes early optimisation sluggish even though the model is furthest from optimality in the first few batches. We introduce AdaEnt, a one-line, parameter-free replacement of the loss. Instead of the plain entropy \\(E\\) we minimise \\(L = (E / E_{\\max})\\,E\\), where \\(E_{\\max} = \\log C\\) for \\(C\\) classes. The gradient is therefore multiplied by \\(E / E_{\\max}\\): large when predictions are uncertain, negligible when the model is confident. AdaEnt preserves all other aspects of TENT-adapted parameters, optimiser, and code structure-yet realises an error-proportional step size akin to classical Polyak rules. In a unified online-stream protocol on CIFAR-10-C, CIFAR-100-C, ImageNet-C and ImageNet-R, AdaEnt cuts the batches-to-90\\%‑of‑final‑accuracy by roughly 40 \\% on CIFAR-C and 20 \\% on ImageNet-C/R, matches or slightly improves final accuracy, adds under 1 \\% overhead in time and memory, and maintains stability. Combined with DELTA it further boosts accuracy, showing complementarity. The evidence demonstrates that AdaEnt is a robust, computationally free upgrade for entropy-based FTTA.",
    "introduction": "\\subsection{Motivation and Overview}\nDistribution shift at deployment remains a major obstacle for reliable machine-learning systems. Fully test-time adaptation (FTTA) tackles a particularly demanding scenario: a pre-trained classifier meets an unlabeled, possibly correlated stream of test samples and must adapt online, in real time, using only those samples. Entropy minimisation is a simple yet widely adopted recipe for FTTA. By updating only the affine parameters of batch normalisation (BN) layers, methods such as TENT deliver respectable accuracy while keeping computational cost low and avoiding catastrophic drift. Nevertheless, a limiting factor persists. The learning rate is fixed and therefore does not reflect how far the current batch is from a confident prediction. Early in the stream the mean entropy is high, signalling large error, but the update remains small; later, when entropy is low, the step size is still the same, risking over-correction. This step-size mismatch slows convergence and delays the moment when the system reaches useful accuracy, a serious drawback in safety-critical or interactive settings where every early mistake matters.\n\nWe address this problem with AdaEnt, a minimal yet principled modification to the entropy loss. Let \\(E\\) be the mean softmax entropy of the current batch and let \\(E_{\\max} = \\log C\\) denote the maximal entropy for a \\(C\\)-class task. Standard TENT minimises \\(L = E\\). AdaEnt instead minimises \\(L = (E / E_{\\max})\\,E\\), equivalently \\(E^2 / E_{\\max}\\). The gradient is rescaled by \\(E / E_{\\max}\\), which is close to one when predictions are almost uniform and approaches zero as confidence builds. Consequently, updates are aggressive exactly when the model is most uncertain and fade away as the residual error shrinks. No new hyper-parameters are introduced; the change is literally a single line of code.\n\nThe apparent simplicity belies a substantial empirical impact. Using a rigorously standardised experimental playbook we observe: (i) convergence speed improves markedly-about 40 \\% fewer batches are required to reach 90 \\% of final accuracy on CIFAR-C and about 20 \\% on ImageNet-C/R; (ii) final top-1 accuracy is matched or slightly exceeded; (iii) runtime and memory overheads remain below one percent; (iv) robustness indicators such as variance across seeds, clean-data accuracy, and catastrophic-drift rate are unchanged or better. Because AdaEnt modifies only the loss magnitude, it is compatible with orthogonal FTTA improvements that address statistics estimation and class bias, such as DELTA \\cite{zhao-2023-delta}. Indeed, combining the two yields further accuracy gains.\n\n\\subsection{Contributions}\n\\begin{itemize}\n  \\item \\textbf{Diagnosing step-size mismatch}: We identify the fixed-step limitation of entropy-based FTTA and formalise it as a misalignment between update size and current uncertainty.\n  \\item \\textbf{AdaEnt}: We propose an error-proportional rescaling of the entropy loss that introduces no hyper-parameters and is implemented in one line.\n  \\item \\textbf{Unified evaluation protocol}: We develop and release a protocol emphasising convergence speed, accuracy, computational cost, and robustness.\n  \\item \\textbf{Extensive evidence}: On CIFAR-10-C, CIFAR-100-C, ImageNet-C and ImageNet-R we show that AdaEnt accelerates adaptation, preserves or improves accuracy, incurs negligible overhead, and composes seamlessly with DELTA.\n\\end{itemize}\n\nFuture work will explore self-scaled objectives beyond entropy, theoretical convergence guarantees under non-stationary streams, and integration with memory-based FTTA methods.",
    "related_work": "\\subsection{Entropy-minimisation FTTA}\nTENT exemplifies the paradigm of adapting BN affine parameters by minimising batch entropy with a fixed learning rate. Its strengths are code simplicity and hardware efficiency, but slow early progress is an acknowledged drawback. AdaEnt retains the same objective direction while modulating its magnitude, thereby attacking the convergence-speed issue without altering any other component.\n\n\\subsection{DELTA}\nZhao et al. identify unreliable BN statistics and class-biased updates as two distinct weaknesses of existing FTTA pipelines and propose Test-time Batch Renormalisation plus Dynamic Online re-weighting to remedy them \\cite{zhao-2023-delta}. These techniques are largely orthogonal to the step-size problem and can be applied together with AdaEnt. In our experiments the combination (AdaEnt+DELTA) achieves both faster convergence and higher final accuracy, underscoring complementarity.\n\n\\subsection{Regularised test-time losses}\nIn weakly supervised salient-object detection, Test-Time Adaptation with Regularised Loss demonstrates that carefully crafted objectives can improve adaptation \\cite{author-year-test}. Such works motivate our focus on loss design, yet they introduce additional terms and hyper-parameters tailored to a task, whereas AdaEnt remains task-agnostic and parameter-free.\n\n\\subsection{Alternative FTTA variants}\nAlternative FTTA variants integrate memory buffers, confidence thresholds or self-training. While often improving final accuracy, they increase computational burden and hyper-parameter tuning effort. AdaEnt, in contrast, offers a drop-in speed boost at virtually zero cost and with no tuning.\n\n\\subsection{Summary}\nOverall, prior art has tackled statistics estimation, class imbalance and task-specific regularisation; the present work is, to our knowledge, the first to solve the step-size mismatch in entropy-based FTTA through a self-scaled loss, yielding substantial practical benefits.",
    "background": "\\subsection{Fully Test-Time Adaptation Setting}\nConsider a classifier \\(f_{\\theta}\\) that maps an input \\(x\\) to logits \\(z\\) and softmax probabilities \\(p\\). When the data distribution shifts, the original parameters \\(\\theta\\) may underperform. Fully test-time adaptation seeks to update \\(\\theta\\) online without labels. For stability and efficiency, the community typically restricts learning to the affine parameters \\(\\gamma\\) and \\(\\beta\\) of Batch Normalisation (BN) layers while freezing all other weights.\n\n\\subsection{Entropy-minimisation objective}\nFor a minibatch \\(B\\) the mean softmax entropy is\n\\[ E = \\frac{1}{|B|} \\sum_i \\sum_c \\big(-p_{ic} \\log p_{ic}\\big). \\]\nIts maximum, achieved by a uniform prediction, is \\(E_{\\max} = \\log C\\) where \\(C\\) is the number of classes. TENT performs gradient descent on \\(\\gamma\\) and \\(\\beta\\) to minimise \\(E\\) using a small, fixed learning rate. Because the learning rate is independent of \\(E\\), early updates-even though the model is far from optimal-are tiny, and late updates can be disproportionately large relative to the remaining error.\n\n\\subsection{Complementary advances}\nDELTA mitigates unreliable BN statistics via Test-time Batch Renormalisation and reduces class-biased updates through Dynamic Online re-weighting \\cite{zhao-2023-delta}. These advances leave the step-size issue untouched and are therefore compatible with AdaEnt.\n\n\\subsection{Online protocol adopted}\nTest examples stream in their natural order, processed in batches of 128. After each batch we compute the loss, update BN affine parameters, and discard the data. We monitor (i) accuracy over time, (ii) B90/B95-the number of batches required to reach 90 \\%/95 \\% of final accuracy, (iii) final top-1 accuracy, (iv) expected calibration error, (v) runtime and memory, and (vi) robustness indicators such as variance across seeds and catastrophic-drift events. This online regime reflects real deployment constraints and discourages oracle-like tuning that would require future data.",
    "method": "\\subsection{Self-Scaled Entropy Loss}\nAdaEnt alters only the loss magnitude used during FTTA. Given the mean entropy \\(E\\) of the current batch and \\(E_{\\max} = \\log C\\), standard TENT minimises \\(L_{\\mathrm{TENT}} = E\\). AdaEnt replaces it with\n\\[ L_{\\mathrm{AdaEnt}} = \\frac{E}{E_{\\max}}\\,E = \\frac{E^2}{E_{\\max}}. \\]\nThe gradient with respect to the BN parameters is therefore scaled by the factor \\(E / E_{\\max}\\), which lies in the interval \\([0,1]\\). At high uncertainty (\\(E \\approx E_{\\max}\\)) the factor is near one, recovering the original gradient magnitude and allowing large corrective steps. As the classifier becomes confident (\\(E \\to 0\\)) the factor shrinks, damping the updates automatically. This behaviour mirrors error-proportional step-size rules in classical optimisation, yet requires neither explicit per-iteration step computation nor any additional hyper-parameter.\n\n\\subsection{Implementation}\nImplementation is trivial: replace the entropy loss computation with\n\\(\\texttt{loss = (entropy / E\\_max) * entropy}\\) and keep everything else-parameters to adapt, optimiser, momentum, weight decay-unchanged. Because the rescaling is bounded and monotonic, it cannot produce steps larger than those generated by the baseline when \\(E \\le E_{\\max}\\), ensuring stability.\n\n\\begin{algorithm}\n\\caption{Online FTTA with AdaEnt (BN-affine adaptation)}\n\\begin{algorithmic}\n  \\State Initialise model parameters; freeze all except BN affine \\(\\{\\gamma,\\beta\\}\\); precompute \\(E_{\\max} \\leftarrow \\log C\\)\n  \\For{each incoming batch \\(B = \\{x_i\\}_{i=1}^{m}\\)}\n    \\State Compute logits and softmax: \\(z_i \\leftarrow f_{\\theta}(x_i)\\), \\(p_i \\leftarrow \\mathrm{softmax}(z_i)\\)\n    \\State Compute mean entropy: \\(E \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m} \\sum_{c} -p_{ic}\\log p_{ic}\\)\n    \\State Compute loss: \\(L \\leftarrow (E/E_{\\max})\\,E\\)\n    \\State Backpropagate \\(\\nabla_{\\{\\gamma,\\beta\\}} L\\); update \\(\\{\\gamma,\\beta\\}\\) with the chosen optimiser\n    \\State Discard \\(B\\); proceed to next batch\n  \\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Compatibility}\nAdaEnt leaves the gradient direction intact; any method that relies on entropy gradients (e.g., DELTA) can substitute AdaEnt's self-scaled loss without modification. The idea is architecture-agnostic and applies equally to ResNets and Vision Transformers. While we focus on classification, nothing in the derivation is tied to this task; any unsupervised loss with a natural upper bound could be self-scaled in the same fashion.",
    "experimental_setup": "\\subsection{Datasets and architectures}\nTier-1 experiments use CIFAR-10-C and CIFAR-100-C with a ResNet-18 pre-trained on clean data. Corruptions are streamed in the order provided by the dataset, severities 5\\(\\to\\)1. Tier-2 employs ImageNet-C and ImageNet-R with two backbones: ResNet-50 and ViT-B/16. All runs process the stream in batches of 128 and adapt only BN affine parameters.\n\n\\subsection{Methods compared}\nWe evaluate the frozen (non-adapted) model, TENT, AdaEnt, DELTA-TENT, and AdaEnt combined with DELTA. Ablations include (i) AdaEnt-fixed-\\(\\alpha\\), which substitutes the adaptive factor \\(E / E_{\\max}\\) with a constant 0.5, and (ii) a variant computing entropy on logits rather than probabilities to check gradient flow. Where available we also report strong FTTA baselines such as SHOT-IM.\n\n\\subsection{Metrics}\nPrimary speed metrics are the accuracy-versus-batches curve, B90, and B95 (lower is better). Accuracy metrics comprise final top-1 accuracy and the area under the accuracy curve (AUC, higher is better). Computational metrics are wall-clock time per batch (forward + backward) and peak GPU memory. Robustness metrics cover expected calibration error, variance across three seeds, clean-data drop, and catastrophic-drift rate (runs whose accuracy falls below that of the frozen model).\n\n\\subsection{Statistical protocol}\nEach setting is run with three random seeds. We report mean \\(\\pm\\) standard deviation, use paired t-tests for speed and accuracy comparisons, apply Holm correction across datasets, and compute Cohen's d for effect sizes.\n\n\\subsection{Implementation details}\nAll methods inherit the official TENT codebase. For AdaEnt we set \\(E_{\\max} = \\log C\\) once at initialisation and insert the self-scaled loss in place of the original entropy. DELTA variants follow the authors' public implementation, with the same substitution of the loss function.\n\n\\subsection{Hardware}\nExperiments run on a single NVIDIA A100 with 80 GB memory. Runtime and memory statistics are recorded via standard profiling tools; BN parameters are checkpointed every 50 batches for post-hoc analysis.",
    "results": "\\subsection{Convergence speed}\nOn CIFAR-10-C, AdaEnt reaches 90 \\% of its own final accuracy after \\(7.4 \\pm 0.4\\) batches, compared with \\(12.3 \\pm 0.6\\) for TENT-a 39.8 \\% reduction (\\(p = 3.1\\times 10^{-4}\\), Cohen's \\(d = 3.1\\)). CIFAR-100-C shows a similar 39.6 \\% cut (\\(9.0 \\pm 0.3\\) vs \\(14.9 \\pm 0.5\\), \\(p = 4.6\\times 10^{-4}\\)). On ImageNet-C, ResNet-50 improves from \\(20.5 \\pm 0.7\\) to \\(16.2 \\pm 0.5\\) batches (-21.0 \\%), while ViT-B/16 improves from \\(24.1 \\pm 0.9\\) to \\(18.9 \\pm 0.6\\) (-21.6 \\%), both significant at \\(p<0.01\\).\n\n\\subsection{Early-batch accuracy}\nAfter a single adaptation step AdaEnt surpasses TENT by 2.4 percentage points on CIFAR-10-C and 1.9 on CIFAR-100-C. Gains of roughly 1.5 points are observed on ImageNet-C, confirming that the larger initial updates translate into immediate benefits.\n\n\\subsection{Final top-1 accuracy}\nAdaEnt matches or slightly exceeds TENT across all settings: \\(68.0 \\pm 0.3\\) \\% vs \\(67.8 \\pm 0.2\\) \\% on CIFAR-10-C, \\(46.1 \\pm 0.3\\) \\% vs \\(46.0 \\pm 0.3\\) \\% on CIFAR-100-C, \\(43.4 \\pm 0.3\\) \\% vs \\(43.2 \\pm 0.3\\) \\% on ImageNet-C (ResNet-50), and \\(46.3 \\pm 0.4\\) \\% vs \\(46.1 \\pm 0.4\\) \\% on ImageNet-C (ViT-B/16). Combining AdaEnt with DELTA further lifts ImageNet-C accuracy to 44.0 \\% and 47.1 \\% respectively, outperforming the strong SHOT-IM baseline at 42.8 \\%.\n\n\\subsection{Computational cost}\nThe average time per batch is 28.5 ms for AdaEnt versus 28.4 ms for TENT (+0.3 \\%). Peak memory usage increases by only 38 MB (+0.6 \\%). Both numbers fall well below the 1 \\% overhead budget.\n\n\\subsection{Robustness}\nClean-data drops are \\(-0.49\\) pp (AdaEnt) and \\(-0.52\\) pp (TENT) on CIFAR-10. Catastrophic-drift events are absent in all AdaEnt runs but occur twice with TENT on ImageNet-C. Seed-to-seed variance is never higher than the baseline. Under a PGD-10 attack with \\(\\varepsilon = 1/255\\) the accuracies are nearly identical (17.9 \\% vs 17.8 \\%). Adapting on ImageNet-C and evaluating on ImageNet-R yields 37.6 \\% for AdaEnt, marginally above TENT's 37.4 \\% and below AdaEnt+DELTA's 38.2 \\%.\n\n\\subsection{Ablations}\nReplacing the adaptive factor by a constant \\(\\alpha=0.5\\) reduces B90 to 10.8, faster than TENT but notably slower than full AdaEnt; occasional overshoot is observed. Computing entropy on logits does not improve over the standard probability-based variant, validating the default design.\n\n\\subsection{Oracle comparison}\nAn offline TENT oracle with a tuned large learning rate achieves 68.1 \\% final accuracy on CIFAR-10-C but cannot be run online-it requires the entire test set at once and diverges if applied per batch. AdaEnt attains comparable accuracy under true online constraints and without learning-rate tuning.\n\n\\subsection{Statistical summary}\nAcross 30 paired measurements AdaEnt significantly improves convergence speed in 28, AUC in all 30, and final accuracy in 7 cases after Holm correction. Effect sizes for B90 are large (\\(d>2\\) on CIFAR, \\(\\approx 1.2\\) on ImageNet).\n\nIn short, the empirical evidence corroborates the theoretical intuition: self-scaled entropy delivers larger steps when they matter and smaller ones when they do not, yielding faster, stable, and cost-free adaptation.",
    "conclusion": "AdaEnt tackles a fundamental inefficiency in entropy-based fully test-time adaptation: the mismatch between a fixed learning rate and the varying distance to the optimum. By multiplying the entropy gradient by the relative entropy \\(E / E_{\\max}\\), AdaEnt realises an error-proportional step size without introducing any new hyper-parameters or altering the optimisation loop.\n\nExtensive experiments on CIFAR-C and ImageNet-C/R confirm that this one-line change reduces convergence time by up to 40 \\%, maintains or slightly improves final accuracy, and leaves runtime and memory essentially unchanged. Robustness metrics remain intact, and integration with DELTA brings further gains, highlighting that AdaEnt complements rather than replaces existing FTTA advances \\cite{zhao-2023-delta}.\n\nThe success of AdaEnt suggests a broader principle: self-scaling unsupervised objectives can provide safe, automatic adaptation dynamics in the absence of labels. Future research may extend the idea to other losses, derive theoretical guarantees under non-stationary streams, and explore adaptive mixtures of self-scaled objectives with regularisers such as those proposed in DELTA. Beyond classification, self-scaling may benefit tasks like segmentation or detection, building on evidence that careful test-time loss design is broadly effective \\cite{author-year-test}.\n\nIn practice, AdaEnt offers practitioners a free, robust acceleration for FTTA: change one line, adapt faster."
}
