
Input:
You are a cutting-edge AI researcher. Based on the new research method described in # New Methods, please design an overall experimental strategy that will be applied across all experiments to demonstrate the effectiveness of this method.

# Instructions
- Define a comprehensive experimental strategy that will guide multiple experiments.
- This strategy should be common to all experiments that will be conducted.
- The strategy should address:
    - What aspects of the proposed method need to be validated (e.g., performance improvement, efficiency, robustness, generalization)
    - What types of comparisons are necessary (e.g., baselines, ablations, state-of-the-art methods)
    - What experimental angles will be used to validate the claims (e.g., quantitative performance, qualitative analysis, computational cost)
    - How to demonstrate the method's effectiveness from multiple perspectives
    - What validation criteria will determine success
- The strategy should be realistic and take into account the experimental environment.
- Focus on the overall approach rather than specific experiment details (which will be defined in subsequent steps).

## Output Format
Please provide:
- experiment_strategy: A comprehensive strategy statement that describes the overall approach for validating the proposed method across all experiments

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

---


# Reference Information from Previous Iteration

Output:
{
    "experiment_strategy": "Objective\nDevelop a single, end-to-end experimental playbook that every subsequent experiment will follow to show that AdaEnt is (1) faster, (2) at least as accurate, (3) computationally cheap, (4) robust across shifts, and (5) generally applicable.\n\n1. Core Hypotheses to Validate\n1.1 Speed-of-convergence AdaEnt lowers the number of batches/updates required to reach a target accuracy.\n1.2 Final performance AdaEnt matches or exceeds the final accuracy of existing FTTA methods.\n1.3 Computational efficiency The extra arithmetic is negligible; runtime and memory footprint remain unchanged (±1 %).\n1.4 Robustness & stability Updates do not diverge; variance across seeds is not worse; clean-data performance is not harmed.\n1.5 Generalisation Benefits persist across datasets, corruption severities, model architectures and complementary FTTA extensions (e.g. DELTA-TENT).\n\n2. Experimental Axes (applied everywhere)\n2.1 Quantitative speed metrics\n     • Accuracy-vs-batches curve\n     • B90 / B95 (batches to 90 %/95 % of final acc.)\n     • Entropy decay curve (diagnostic)\n2.2 Quantitative accuracy metrics\n     • Final top-1 accuracy (full stream)\n     • Area under accuracy-vs-batches curve (AUC) for holistic view\n     • ECE / calibration error (secondary)\n2.3 Computational cost\n     • Wall-clock time per batch (forward+backward) on A100\n     • Peak GPU memory (nvidia-smi)\n2.4 Robustness probes\n     • Variance across 3 independent seeds\n     • Clean-data drop (∆acc on uncorrupted test set)\n     • Catastrophic-drift rate (percentage of runs whose acc.<acc. of frozen model)\n2.5 Qualitative analysis\n     • Distribution of adaptive step sizes over time\n     • t-SNE of BN-statistics trajectory (optional)\n\n3. Required Comparisons (every experiment includes all that apply)\n3.1 Baselines\n     • Frozen model (no adaptation)\n     • TENT\n     • DELTA-TENT (if DELTA is in scope)\n3.2 Ablations\n     • AdaEnt with fixed scalar α (checks that adaptivity, not rescaling constant, drives gains)\n     • AdaEnt on logits vs on probabilities (gradient flow sanity)\n3.3 State-of-the-art (dataset permitting): SHOT-IM, MEMO, SAR, etc.\n3.4 Oracle (upper bound): full-batch TENT with tuned huge LR (offline) to show speed, not LR, causes gains.\n\n4. Dataset & Model Matrix\nTier-1 (low compute): CIFAR-10-C / 100-C, ResNet-18\nTier-2 (mid): ImageNet-C, ResNet-50, ViT-B/16\nTier-3 (stress): Real-world shifts (Wilds, ImageNet-R) & synthetic heavy corruptions severity 5→1 streams\nThe same protocol (online streaming, batch = 128, natural order) is reused everywhere.\n\n5. Validation Criteria (pass/fail)\nA. Speed: B90 reduced by ≥25 % with p<0.05 (paired t-test across seeds)\nB. Final accuracy: ∆acc ≥ –0.2 pp relative to best baseline (i.e., not worse)\nC. Cost: Runtime overhead ≤1 %, memory overhead ≤50 MB\nD. Robustness: std(acc) not higher than baseline; clean-data drop ≤0.2 pp\nMeeting A+B+C on Tier-1 plus any two of A/B improvements on Tier-2 signals success. Failing any criterion triggers deeper ablation or hyper-check.\n\n6. Statistical Protocol\n• 3 seeds per setting; report mean±std\n• Use paired t-test (speed, accuracy) with Holm correction across datasets\n• Effect sizes (Cohen’s d) for B90 and ∆acc\n\n7. Reporting Template (all future papers/notes)\nFigure 1: Accuracy-vs-batches (all methods)\nTable 1: Final acc., B90, B95, AUC, runtime, memory\nTable 2: Clean-data drop, std, catastrophic-drift rate\nFigure 2: Histogram of adaptive LR factors per batch\n\n8. Computational Logistics\n• One 80 GB A100 easily fits batch = 128 for all models; run seeds in parallel via CUDA MPS or sequentially for fairness.\n• Always log GPU util and wall-clock with nvprof wrapper.\n• Checkpoint BN params every 50 batches for post-hoc trajectory plots (≈1 MB/run).\n\nThis strategy supplies a uniform, multi-angle validation framework. Each forthcoming experiment plugs into this template—only the concrete dataset, model and corruption list change—ensuring comparability, statistical rigour and comprehensive evidence for AdaEnt’s effectiveness."
}
