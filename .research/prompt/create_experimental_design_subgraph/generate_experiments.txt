
Input:
You are a cutting-edge AI researcher. Based on the new method described in # New Methods and the experimental policy outlined in # Experiment Strategy, please generate 2 distinct Experiment objects.

# Instructions
- Generate 2 major experimental lines (Experiment objects) based on the experimental strategy.
- Each Experiment (identified by experiment_id) represents a different experimental perspective or validation angle.
- Within each Experiment, run_variations are the variations that will be compared against each other (e.g., ["baseline", "proposed"], ["full-method", "ablation-A", "ablation-B"]).
- Keep run_variations to 3-5 variations per experiment (including baseline and proposed method) to ensure reasonable execution time and resource usage.
- Each Experiment should:
    - Have a unique experiment_id (e.g., "exp-1", "exp-2", "exp-3")
    - Have a clear description of its objective or hypothesis
    - Have a list of run_variations that will be compared within this experiment
    - Cover different aspects of validating the proposed method
- The experiments should be complementary and cover various validation angles such as:
    - Main performance validation
    - Ablation studies
    - Robustness tests
    - Comparison with baselines
    - Hyperparameter sensitivity analysis
    - Computational efficiency analysis
- Each experiment will have its own GitHub branch and code.
- The run_variations within each experiment define different configurations or conditions to test (e.g., different hyperparameters, different baselines, different datasets).

- Design the details of each experiment assuming the execution environment specified in "Experimental Environment."
- The experimental details should include the following for each experiment:
    - Machine learning / deep learning models to be used
        - If necessary, also include baseline models.
    - Datasets
    - Dataset preprocessing methods
    - Data splitting method (train/val/test, cross-validation)
    - Number of repetitions (number of seeds), averaging method, and selection criteria (best-val, last, early stopping)
    - Evaluation metrics
        - Primary and secondary metrics
        - Examples: Accuracy / F1 / AUROC (classification), RMSE / MAE (regression), mAP (detection), mIoU (segmentation), BLEU / ROUGE / METEOR (generation), NDCG / MRR (ranking), ECE / Brier Score (calibration)
    - Comparisons
        - Prior methods (strong baselines, SOTA, simple baselines), etc.
        - If there are implementation or configuration differences, note the adjustments in footnotes.
    - Methods for analyzing important hyperparameters (e.g., learning rate, temperature, k, thresholds)
    - Methods for assessing robustness
        - Resistance to noise injection, distribution shift (OOD), adversarial perturbations, and domain transfer
    - Computation of FLOPs, training/inference time, memory usage, and cost / wall-clock time
    - Example experimental code
- Avoid excessive redundancy across experiments. When a single experiment can cover multiple validation items, integrate them appropriately.
- NO-FALLBACK CONSTRAINT: Never suggest using synthetic/dummy/placeholder data.
- Also provide:
    - expected_models: A list of specific model names/architectures that will be used across all experiments (e.g., ["ResNet-50", "BERT-base", "GPT-3.5-turbo"])
    - expected_datasets: A list of specific dataset names that will be used across all experiments (e.g., ["CIFAR-10", "ImageNet", "IMDB Reviews"])

## Output Format
Please provide:
- experiments: A list of 2 Experiment objects, each with:
    - experiment_id: Unique identifier
    - run_variations: List of variation names/identifiers for this experiment
    - description: Detailed description including all aspects mentioned in the instructions
- expected_models: List of model names/architectures
- expected_datasets: List of dataset names

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

# Experiment Strategy
Objective
Develop a single, end-to-end experimental playbook that every subsequent experiment will follow to show that AdaEnt is (1) faster, (2) at least as accurate, (3) computationally cheap, (4) robust across shifts, and (5) generally applicable.

1. Core Hypotheses to Validate
1.1 Speed-of-convergence AdaEnt lowers the number of batches/updates required to reach a target accuracy.
1.2 Final performance AdaEnt matches or exceeds the final accuracy of existing FTTA methods.
1.3 Computational efficiency The extra arithmetic is negligible; runtime and memory footprint remain unchanged (±1 %).
1.4 Robustness & stability Updates do not diverge; variance across seeds is not worse; clean-data performance is not harmed.
1.5 Generalisation Benefits persist across datasets, corruption severities, model architectures and complementary FTTA extensions (e.g. DELTA-TENT).

2. Experimental Axes (applied everywhere)
2.1 Quantitative speed metrics
     • Accuracy-vs-batches curve
     • B90 / B95 (batches to 90 %/95 % of final acc.)
     • Entropy decay curve (diagnostic)
2.2 Quantitative accuracy metrics
     • Final top-1 accuracy (full stream)
     • Area under accuracy-vs-batches curve (AUC) for holistic view
     • ECE / calibration error (secondary)
2.3 Computational cost
     • Wall-clock time per batch (forward+backward) on A100
     • Peak GPU memory (nvidia-smi)
2.4 Robustness probes
     • Variance across 3 independent seeds
     • Clean-data drop (∆acc on uncorrupted test set)
     • Catastrophic-drift rate (percentage of runs whose acc.<acc. of frozen model)
2.5 Qualitative analysis
     • Distribution of adaptive step sizes over time
     • t-SNE of BN-statistics trajectory (optional)

3. Required Comparisons (every experiment includes all that apply)
3.1 Baselines
     • Frozen model (no adaptation)
     • TENT
     • DELTA-TENT (if DELTA is in scope)
3.2 Ablations
     • AdaEnt with fixed scalar α (checks that adaptivity, not rescaling constant, drives gains)
     • AdaEnt on logits vs on probabilities (gradient flow sanity)
3.3 State-of-the-art (dataset permitting): SHOT-IM, MEMO, SAR, etc.
3.4 Oracle (upper bound): full-batch TENT with tuned huge LR (offline) to show speed, not LR, causes gains.

4. Dataset & Model Matrix
Tier-1 (low compute): CIFAR-10-C / 100-C, ResNet-18
Tier-2 (mid): ImageNet-C, ResNet-50, ViT-B/16
Tier-3 (stress): Real-world shifts (Wilds, ImageNet-R) & synthetic heavy corruptions severity 5→1 streams
The same protocol (online streaming, batch = 128, natural order) is reused everywhere.

5. Validation Criteria (pass/fail)
A. Speed: B90 reduced by ≥25 % with p<0.05 (paired t-test across seeds)
B. Final accuracy: ∆acc ≥ –0.2 pp relative to best baseline (i.e., not worse)
C. Cost: Runtime overhead ≤1 %, memory overhead ≤50 MB
D. Robustness: std(acc) not higher than baseline; clean-data drop ≤0.2 pp
Meeting A+B+C on Tier-1 plus any two of A/B improvements on Tier-2 signals success. Failing any criterion triggers deeper ablation or hyper-check.

6. Statistical Protocol
• 3 seeds per setting; report mean±std
• Use paired t-test (speed, accuracy) with Holm correction across datasets
• Effect sizes (Cohen’s d) for B90 and ∆acc

7. Reporting Template (all future papers/notes)
Figure 1: Accuracy-vs-batches (all methods)
Table 1: Final acc., B90, B95, AUC, runtime, memory
Table 2: Clean-data drop, std, catastrophic-drift rate
Figure 2: Histogram of adaptive LR factors per batch

8. Computational Logistics
• One 80 GB A100 easily fits batch = 128 for all models; run seeds in parallel via CUDA MPS or sequentially for fairness.
• Always log GPU util and wall-clock with nvprof wrapper.
• Checkpoint BN params every 50 batches for post-hoc trajectory plots (≈1 MB/run).

This strategy supplies a uniform, multi-angle validation framework. Each forthcoming experiment plugs into this template—only the concrete dataset, model and corruption list change—ensuring comparability, statistical rigour and comprehensive evidence for AdaEnt’s effectiveness.

---


# Reference Information from Previous Iteration

Output:
{
    "experiments": [
        {
            "experiment_id": "exp-1-cifar-core",
            "run_variations": [
                "frozen",
                "TENT",
                "AdaEnt",
                "AdaEnt-fixed-α",
                "oracle-fullbatch-LR"
            ],
            "description": "Objective: Core performance & convergence-speed validation of AdaEnt on low-compute Tier-1 benchmarks (CIFAR-10-C / 100-C) with ResNet-18.\n\nModels:\n• Pre-trained ResNet-18 (BN layers unfrozen for adaptation)\n• Same backbone for all variations to isolate loss effect.\n\nDatasets:\n• CIFAR-10-C, CIFAR-100-C – 15 corruption types × 5 severities.\n• Clean CIFAR-10/100 test sets for clean-data drop.\n\nPre-processing:\n• Standard 32×32 centre-crop & per-channel mean/std normalisation.\n• No data augmentation during TTA.\n\nData splitting & streaming:\n• Entire corrupted test set is streamed once in natural order with batch_size=128 (online). No access to labels.\n• Clean test set is evaluated once before & after adaptation.\n\nRun repetitions:\n• 3 random seeds (independent shuffles of stream start index).\n• Metrics reported as mean±std; selection = last checkpoint (online scenario has no early stop).\n\nEvaluation metrics:\nPrimary – Top-1 accuracy, B90, B95, AUC(acc-vs-batches).\nSecondary – Expected Calibration Error (ECE, 15 bins), wall-clock per batch, peak GPU mem.\n\nComparisons / run_variations:\n1. frozen – no adaptation (upper-bound on speed, lower-bound on acc.)\n2. TENT – baseline.\n3. AdaEnt – proposed.\n4. AdaEnt-fixed-α – ablation: replace adaptive factor with static α=0.5.\n5. oracle-fullbatch-LR – offline TENT with tuned large LR (upper-bound on final acc.).\n\nHyper-parameter analysis:\n• Sweep α∈{0.25,0.5,1.0} for variation 4 (separate grid not part of main run_variations) and LR∈{1e-3,3e-3,1e-2} for oracle; report sensitivity curves.\n\nRobustness checks:\n• Noise injection: add iid Gaussian noise σ=0.05 to inputs for last 10% of stream.\n• OOD: evaluate on corruption severity 1 after training on severity 5→1.\n• Variance across seeds & catastrophic-drift rate.\n\nComputational efficiency:\n• Wrap training loop with NVTX, use nvprof to record FLOPs, wall-clock, memory.\n• Report overhead relative to TENT.\n\nExample code excerpt (PyTorch):\n```python\nfor x in stream_loader:\n    x = x.cuda(non_blocking=True)\n    logits = model(x)\n    entropy = softmax_entropy(logits).mean(0)\n    loss = entropy*entropy/math.log(num_classes)  # AdaEnt\n    loss.backward(); optim.step(); optim.zero_grad()\n```\n\nExpected outcome: AdaEnt cuts B90 by ≥25 % w.r.t. TENT, keeps final accuracy within ±0.1 pp, runtime overhead <1 %."
        },
        {
            "experiment_id": "exp-2-imagenet-robust",
            "run_variations": [
                "TENT",
                "AdaEnt",
                "AdaEnt+DELTA",
                "SHOT-IM",
                "oracle-large-LR"
            ],
            "description": "Objective: Robustness, scalability and cross-architecture generalisation on Tier-2 datasets (ImageNet-C, ImageNet-R) using ResNet-50 and ViT-B/16.\n\nModels:\n• ResNet-50 (BN-adaptable).\n• Vision Transformer ViT-B/16 (LayerNorm adapt. via DELTA when enabled).\n\nDatasets:\n• ImageNet-C (1.3 M images, 15 corruptions × severities 5→1).\n• ImageNet-R for real-world domain shift.\n• Original ImageNet-val (clean) for clean-data drop.\n\nPre-processing:\n• Resize-shorter-side 256 → center-crop 224; normalise w/ ImageNet mean/std.\n\nData splitting & streaming:\n• Same online protocol: corruption severity 5 images streamed first, decreasing to 1, batch_size=128.\n• For ImageNet-R, stream once after finishing ImageNet-C adaptation (tests domain-transfer stability).\n\nRun repetitions:\n• 3 seeds (random starting corruption). Report mean±std; last checkpoint.\n\nEvaluation metrics:\nPrimary – Top-1 accuracy, B90, AUC.\nSecondary – mCE (mean Corruption Error), ECE, FLOPs/batch, GPU util %.\n\nrun_variations details:\n1. TENT – baseline.\n2. AdaEnt – proposed.\n3. AdaEnt+DELTA – AdaEnt loss + DELTA regulariser (λ=0.1).\n4. SHOT-IM – strong SOTA baseline (implementation from official repo, BN layers frozen).\n5. oracle-large-LR – offline TENT with lr=1e-1, full test set at once.\n\nHyper-parameter sensitivity:\n• For AdaEnt+DELTA sweep λ∈{0,0.05,0.1,0.2} (outside main variations) and report curves.\n• Learning-rate sweep for AdaEnt: {1e-4,2e-4,5e-4}.\n\nRobustness probes:\n• Adversarial: PGD-10 ε=1/255 on 1 % sample of stream, measure acc drop.\n• Distribution shift: evaluate adapted model directly on ImageNet-R without further updates.\n• Stability: std(acc) & catastrophic-drift rate across seeds.\n\nComputation profiling:\n• Use PyTorch profiler + nvprof to log FLOPs, time, and memory each 200 batches.\n• Cost analysis: report Δtime and Δmem vs. TENT.\n\nExample code snippet (ViT-B/16 + DELTA):\n```python\nloss_entropy = softmax_entropy(logits).mean(0)\nloss_adaent = loss_entropy*loss_entropy/math.log(num_classes)\nloss_delta = lambda_ * torch.norm(get_bn_stats(model)-running_mean)**2\nloss = loss_adaent + loss_delta\n```\n\nExpected outcome: AdaEnt reduces B90 by ≥20 % on both models; AdaEnt+DELTA attains highest final accuracy (≥0.5 pp over TENT); runtime overhead ≤1 % and memory overhead ≤50 MB; zero catastrophic drifts on ImageNet-R transfer."
        }
    ],
    "expected_models": [
        "ResNet-18",
        "ResNet-50",
        "ViT-B/16"
    ],
    "expected_datasets": [
        "CIFAR-10-C",
        "CIFAR-100-C",
        "ImageNet-C",
        "ImageNet-R"
    ]
}
