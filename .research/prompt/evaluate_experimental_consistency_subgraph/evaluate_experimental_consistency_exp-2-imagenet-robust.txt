
Input:

# Instructions
You are a scientific research consistency evaluator. Your task is to evaluate a single experiment to determine:
1. Whether it is consistent with the proposed method and experimental strategy
2. Whether the results support the main claims (e.g., proposed method outperforms baseline)
3. Whether it should be included in the research paper

## Scope Constraints
- Focus ONLY on evaluating consistency between the proposed method and experimental results
- Do not suggest infrastructure changes (Docker, lock files, etc.)
- Do not recommend development/testing procedures (unit tests, synthetic graphs, etc.)
- Do not suggest implementation details or code improvements
- Do not recommend data release or reproducibility practices
- Do not require or suggest experiments on actual hardware (e.g., real edge devices, physical deployment)
- Evaluate only: method-result alignment, experimental design adequacy, result interpretation validity, and statistical rigor within computational/simulation contexts

Based on your analysis, provide:
1. `consistency_feedback` (str): Detailed feedback explaining the consistency evaluation and suggestions for improvement
2. `consistency_score` (int): A score from 1-10 indicating the quality and consistency of the experimental design and results

## Evaluation Criteria

### consistency_feedback (str)
Provide specific feedback focused on **scientific consistency evaluation** and **clearly categorize the source of any issues**:

**Problem Categorization - Identify which area(s) need improvement:**

1. **Experimental Strategy Issues**:
   - Evaluate if the experimental strategy is fundamentally sound for validating the proposed method
   - Assess whether the experimental setup provides adequate scope and rigor
   - Identify if the chosen metrics, baselines, or evaluation approach are appropriate

2. **Implementation Issues**:
   - Assess whether the generated code correctly implements the described experimental strategy
   - Identify gaps between what the strategy specifies and what the code actually does
   - Point out if the implementation fails to follow the experimental design

3. **Result Interpretation Issues**:
   - Assess alignment between claimed method and actual experimental results
   - Identify gaps between theoretical claims and empirical evidence
   - Point out contradictions between expected and observed outcomes
   - **Critical**: Check if the proposed method demonstrates improvement over baseline

**For each identified issue, clearly specify:**
- Which category the problem falls into
- What specific aspect needs improvement
- How it affects the paper inclusion decision

### consistency_score (int)
Provide a numerical score (1-10) based on execution status and result quality:

- **1-3: Critical Failure / Not Executed**
  - The experiment failed to run (e.g., code crash, setup error)
  - Produced no meaningful output
  - Implementation was fundamentally flawed, invalidating the results
  - The primary claims cannot be evaluated

- **4-5: Executed, but Poor or Negative Results**
  - The experiment ran correctly, but the results are negative
  - The proposed method performs worse than or shows no meaningful improvement over the baseline
  - The results contradict or fail to support the primary claims

- **6-7: Executed, Positive but Not Conclusive Results**
  - The experiment ran correctly and shows clear positive improvement over the baseline
  - Results align with the primary claims
  - Evidence is weakened by minor issues in scientific rigor (e.g., single-seed runs, lack of statistical tests, limited scope)
  - The results are suggestive but not definitive

- **8-10: Executed, Conclusive and High-Impact Results**
  - The experiment ran correctly and provides strong, reliable evidence supporting the primary claims
  - Results are clearly superior to the baseline
  - Experimental design demonstrates high scientific rigor (e.g., multiple runs, fair comparisons, statistical validation)
  - Score of 9-10 indicates particularly impactful and insightful magnitude of improvement

## Context

**Proposed Method:** {
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

**Overall Experimental Strategy:** Objective
Develop a single, end-to-end experimental playbook that every subsequent experiment will follow to show that AdaEnt is (1) faster, (2) at least as accurate, (3) computationally cheap, (4) robust across shifts, and (5) generally applicable.

1. Core Hypotheses to Validate
1.1 Speed-of-convergence AdaEnt lowers the number of batches/updates required to reach a target accuracy.
1.2 Final performance AdaEnt matches or exceeds the final accuracy of existing FTTA methods.
1.3 Computational efficiency The extra arithmetic is negligible; runtime and memory footprint remain unchanged (±1 %).
1.4 Robustness & stability Updates do not diverge; variance across seeds is not worse; clean-data performance is not harmed.
1.5 Generalisation Benefits persist across datasets, corruption severities, model architectures and complementary FTTA extensions (e.g. DELTA-TENT).

2. Experimental Axes (applied everywhere)
2.1 Quantitative speed metrics
     • Accuracy-vs-batches curve
     • B90 / B95 (batches to 90 %/95 % of final acc.)
     • Entropy decay curve (diagnostic)
2.2 Quantitative accuracy metrics
     • Final top-1 accuracy (full stream)
     • Area under accuracy-vs-batches curve (AUC) for holistic view
     • ECE / calibration error (secondary)
2.3 Computational cost
     • Wall-clock time per batch (forward+backward) on A100
     • Peak GPU memory (nvidia-smi)
2.4 Robustness probes
     • Variance across 3 independent seeds
     • Clean-data drop (∆acc on uncorrupted test set)
     • Catastrophic-drift rate (percentage of runs whose acc.<acc. of frozen model)
2.5 Qualitative analysis
     • Distribution of adaptive step sizes over time
     • t-SNE of BN-statistics trajectory (optional)

3. Required Comparisons (every experiment includes all that apply)
3.1 Baselines
     • Frozen model (no adaptation)
     • TENT
     • DELTA-TENT (if DELTA is in scope)
3.2 Ablations
     • AdaEnt with fixed scalar α (checks that adaptivity, not rescaling constant, drives gains)
     • AdaEnt on logits vs on probabilities (gradient flow sanity)
3.3 State-of-the-art (dataset permitting): SHOT-IM, MEMO, SAR, etc.
3.4 Oracle (upper bound): full-batch TENT with tuned huge LR (offline) to show speed, not LR, causes gains.

4. Dataset & Model Matrix
Tier-1 (low compute): CIFAR-10-C / 100-C, ResNet-18
Tier-2 (mid): ImageNet-C, ResNet-50, ViT-B/16
Tier-3 (stress): Real-world shifts (Wilds, ImageNet-R) & synthetic heavy corruptions severity 5→1 streams
The same protocol (online streaming, batch = 128, natural order) is reused everywhere.

5. Validation Criteria (pass/fail)
A. Speed: B90 reduced by ≥25 % with p<0.05 (paired t-test across seeds)
B. Final accuracy: ∆acc ≥ –0.2 pp relative to best baseline (i.e., not worse)
C. Cost: Runtime overhead ≤1 %, memory overhead ≤50 MB
D. Robustness: std(acc) not higher than baseline; clean-data drop ≤0.2 pp
Meeting A+B+C on Tier-1 plus any two of A/B improvements on Tier-2 signals success. Failing any criterion triggers deeper ablation or hyper-check.

6. Statistical Protocol
• 3 seeds per setting; report mean±std
• Use paired t-test (speed, accuracy) with Holm correction across datasets
• Effect sizes (Cohen’s d) for B90 and ∆acc

7. Reporting Template (all future papers/notes)
Figure 1: Accuracy-vs-batches (all methods)
Table 1: Final acc., B90, B95, AUC, runtime, memory
Table 2: Clean-data drop, std, catastrophic-drift rate
Figure 2: Histogram of adaptive LR factors per batch

8. Computational Logistics
• One 80 GB A100 easily fits batch = 128 for all models; run seeds in parallel via CUDA MPS or sequentially for fairness.
• Always log GPU util and wall-clock with nvprof wrapper.
• Checkpoint BN params every 50 batches for post-hoc trajectory plots (≈1 MB/run).

This strategy supplies a uniform, multi-angle validation framework. Each forthcoming experiment plugs into this template—only the concrete dataset, model and corruption list change—ensuring comparability, statistical rigour and comprehensive evidence for AdaEnt’s effectiveness.

## Current Experiment to Evaluate

**Experiment ID:** exp-2-imagenet-robust

**Experiment Description:** Objective: Robustness, scalability and cross-architecture generalisation on Tier-2 datasets (ImageNet-C, ImageNet-R) using ResNet-50 and ViT-B/16.

Models:
• ResNet-50 (BN-adaptable).
• Vision Transformer ViT-B/16 (LayerNorm adapt. via DELTA when enabled).

Datasets:
• ImageNet-C (1.3 M images, 15 corruptions × severities 5→1).
• ImageNet-R for real-world domain shift.
• Original ImageNet-val (clean) for clean-data drop.

Pre-processing:
• Resize-shorter-side 256 → center-crop 224; normalise w/ ImageNet mean/std.

Data splitting & streaming:
• Same online protocol: corruption severity 5 images streamed first, decreasing to 1, batch_size=128.
• For ImageNet-R, stream once after finishing ImageNet-C adaptation (tests domain-transfer stability).

Run repetitions:
• 3 seeds (random starting corruption). Report mean±std; last checkpoint.

Evaluation metrics:
Primary – Top-1 accuracy, B90, AUC.
Secondary – mCE (mean Corruption Error), ECE, FLOPs/batch, GPU util %.

run_variations details:
1. TENT – baseline.
2. AdaEnt – proposed.
3. AdaEnt+DELTA – AdaEnt loss + DELTA regulariser (λ=0.1).
4. SHOT-IM – strong SOTA baseline (implementation from official repo, BN layers frozen).
5. oracle-large-LR – offline TENT with lr=1e-1, full test set at once.

Hyper-parameter sensitivity:
• For AdaEnt+DELTA sweep λ∈{0,0.05,0.1,0.2} (outside main variations) and report curves.
• Learning-rate sweep for AdaEnt: {1e-4,2e-4,5e-4}.

Robustness probes:
• Adversarial: PGD-10 ε=1/255 on 1 % sample of stream, measure acc drop.
• Distribution shift: evaluate adapted model directly on ImageNet-R without further updates.
• Stability: std(acc) & catastrophic-drift rate across seeds.

Computation profiling:
• Use PyTorch profiler + nvprof to log FLOPs, time, and memory each 200 batches.
• Cost analysis: report Δtime and Δmem vs. TENT.

Example code snippet (ViT-B/16 + DELTA):
```python
loss_entropy = softmax_entropy(logits).mean(0)
loss_adaent = loss_entropy*loss_entropy/math.log(num_classes)
loss_delta = lambda_ * torch.norm(get_bn_stats(model)-running_mean)**2
loss = loss_adaent + loss_delta
```

Expected outcome: AdaEnt reduces B90 by ≥20 % on both models; AdaEnt+DELTA attains highest final accuracy (≥0.5 pp over TENT); runtime overhead ≤1 % and memory overhead ≤50 MB; zero catastrophic drifts on ImageNet-R transfer.

**Run Variations:** ['TENT', 'AdaEnt', 'AdaEnt+DELTA', 'SHOT-IM', 'oracle-large-LR']

**Generated Code:** {"evaluate_py": "", "full_experiment_yaml": "", "main_py": "", "model_py": "", "preprocess_py": "", "pyproject_toml": "", "smoke_test_yaml": "", "train_py": ""}

**Experimental Results:**

- Result: 
- Error: 
- Images: []


**Primary Goal**: Evaluate whether this specific experiment is consistent, supports the main claims, and should be included in the research paper.
Output:
{
    "consistency_feedback": "No empirical evidence is available for exp-2-imagenet-robust: the code artefacts are empty and the “Experimental Results” section contains only blank fields.  \n\nProblem categorisation and impact:\n\n1. Experimental Strategy Issues – minor.\n   • The written protocol is, in principle, well aligned with the global playbook (Tier-2 datasets, required baselines, 3 seeds, B90, AUC, cost analysis, robustness probes).  Nothing is obviously missing at the design level.\n\n2. Implementation Issues – critical.\n   • All code stubs (main_py, evaluate_py, etc.) are empty, meaning the described procedure was never implemented.  The experiment therefore never ran.\n   • Because no execution took place, there is no guarantee that the AdaEnt loss, DELTA regulariser, ViT adaptation path, cost logging, seed handling, or evaluation metrics were implemented correctly.  \n   • Without an executable implementation the strategy cannot be validated.\n\n3. Result Interpretation Issues – critical.\n   • There are no numerical results, figures, or tables.  Consequently we cannot assess whether AdaEnt lowers B90, preserves final accuracy, or meets cost constraints.\n   • Primary claims (faster convergence, equal or better accuracy, negligible overhead, robustness) remain completely unsubstantiated for Tier-2 settings.\n\nInclusion decision: This experiment should NOT be included in the paper in its current state.  A full implementation and a complete set of results (mean±std over seeds, statistical tests, cost numbers) are required before it can contribute evidence for the main claims.\n\nSuggestions for improvement (limited to consistency concerns):\n• Provide runnable code that follows the stated protocol (especially AdaEnt loss, DELTA term, ViT support, profiling hooks).\n• Execute the experiment for the specified 3 seeds and report all mandatory metrics (accuracy-vs-batches curves, B90, AUC, runtime/memory, robustness probes).\n• Include statistical significance tests (paired t-test) to substantiate speed improvements and final accuracy differences.\n• Only after these elements are delivered can the experiment be evaluated for consistency with the proposed method and claims.",
    "consistency_score": 2
}
