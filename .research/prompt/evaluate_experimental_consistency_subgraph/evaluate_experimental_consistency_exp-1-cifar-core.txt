
Input:

# Instructions
You are a scientific research consistency evaluator. Your task is to evaluate a single experiment to determine:
1. Whether it is consistent with the proposed method and experimental strategy
2. Whether the results support the main claims (e.g., proposed method outperforms baseline)
3. Whether it should be included in the research paper

## Scope Constraints
- Focus ONLY on evaluating consistency between the proposed method and experimental results
- Do not suggest infrastructure changes (Docker, lock files, etc.)
- Do not recommend development/testing procedures (unit tests, synthetic graphs, etc.)
- Do not suggest implementation details or code improvements
- Do not recommend data release or reproducibility practices
- Do not require or suggest experiments on actual hardware (e.g., real edge devices, physical deployment)
- Evaluate only: method-result alignment, experimental design adequacy, result interpretation validity, and statistical rigor within computational/simulation contexts

Based on your analysis, provide:
1. `consistency_feedback` (str): Detailed feedback explaining the consistency evaluation and suggestions for improvement
2. `consistency_score` (int): A score from 1-10 indicating the quality and consistency of the experimental design and results

## Evaluation Criteria

### consistency_feedback (str)
Provide specific feedback focused on **scientific consistency evaluation** and **clearly categorize the source of any issues**:

**Problem Categorization - Identify which area(s) need improvement:**

1. **Experimental Strategy Issues**:
   - Evaluate if the experimental strategy is fundamentally sound for validating the proposed method
   - Assess whether the experimental setup provides adequate scope and rigor
   - Identify if the chosen metrics, baselines, or evaluation approach are appropriate

2. **Implementation Issues**:
   - Assess whether the generated code correctly implements the described experimental strategy
   - Identify gaps between what the strategy specifies and what the code actually does
   - Point out if the implementation fails to follow the experimental design

3. **Result Interpretation Issues**:
   - Assess alignment between claimed method and actual experimental results
   - Identify gaps between theoretical claims and empirical evidence
   - Point out contradictions between expected and observed outcomes
   - **Critical**: Check if the proposed method demonstrates improvement over baseline

**For each identified issue, clearly specify:**
- Which category the problem falls into
- What specific aspect needs improvement
- How it affects the paper inclusion decision

### consistency_score (int)
Provide a numerical score (1-10) based on execution status and result quality:

- **1-3: Critical Failure / Not Executed**
  - The experiment failed to run (e.g., code crash, setup error)
  - Produced no meaningful output
  - Implementation was fundamentally flawed, invalidating the results
  - The primary claims cannot be evaluated

- **4-5: Executed, but Poor or Negative Results**
  - The experiment ran correctly, but the results are negative
  - The proposed method performs worse than or shows no meaningful improvement over the baseline
  - The results contradict or fail to support the primary claims

- **6-7: Executed, Positive but Not Conclusive Results**
  - The experiment ran correctly and shows clear positive improvement over the baseline
  - Results align with the primary claims
  - Evidence is weakened by minor issues in scientific rigor (e.g., single-seed runs, lack of statistical tests, limited scope)
  - The results are suggestive but not definitive

- **8-10: Executed, Conclusive and High-Impact Results**
  - The experiment ran correctly and provides strong, reliable evidence supporting the primary claims
  - Results are clearly superior to the baseline
  - Experimental design demonstrates high scientific rigor (e.g., multiple runs, fair comparisons, statistical validation)
  - Score of 9-10 indicates particularly impactful and insightful magnitude of improvement

## Context

**Proposed Method:** {
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

**Overall Experimental Strategy:** Objective
Develop a single, end-to-end experimental playbook that every subsequent experiment will follow to show that AdaEnt is (1) faster, (2) at least as accurate, (3) computationally cheap, (4) robust across shifts, and (5) generally applicable.

1. Core Hypotheses to Validate
1.1 Speed-of-convergence AdaEnt lowers the number of batches/updates required to reach a target accuracy.
1.2 Final performance AdaEnt matches or exceeds the final accuracy of existing FTTA methods.
1.3 Computational efficiency The extra arithmetic is negligible; runtime and memory footprint remain unchanged (±1 %).
1.4 Robustness & stability Updates do not diverge; variance across seeds is not worse; clean-data performance is not harmed.
1.5 Generalisation Benefits persist across datasets, corruption severities, model architectures and complementary FTTA extensions (e.g. DELTA-TENT).

2. Experimental Axes (applied everywhere)
2.1 Quantitative speed metrics
     • Accuracy-vs-batches curve
     • B90 / B95 (batches to 90 %/95 % of final acc.)
     • Entropy decay curve (diagnostic)
2.2 Quantitative accuracy metrics
     • Final top-1 accuracy (full stream)
     • Area under accuracy-vs-batches curve (AUC) for holistic view
     • ECE / calibration error (secondary)
2.3 Computational cost
     • Wall-clock time per batch (forward+backward) on A100
     • Peak GPU memory (nvidia-smi)
2.4 Robustness probes
     • Variance across 3 independent seeds
     • Clean-data drop (∆acc on uncorrupted test set)
     • Catastrophic-drift rate (percentage of runs whose acc.<acc. of frozen model)
2.5 Qualitative analysis
     • Distribution of adaptive step sizes over time
     • t-SNE of BN-statistics trajectory (optional)

3. Required Comparisons (every experiment includes all that apply)
3.1 Baselines
     • Frozen model (no adaptation)
     • TENT
     • DELTA-TENT (if DELTA is in scope)
3.2 Ablations
     • AdaEnt with fixed scalar α (checks that adaptivity, not rescaling constant, drives gains)
     • AdaEnt on logits vs on probabilities (gradient flow sanity)
3.3 State-of-the-art (dataset permitting): SHOT-IM, MEMO, SAR, etc.
3.4 Oracle (upper bound): full-batch TENT with tuned huge LR (offline) to show speed, not LR, causes gains.

4. Dataset & Model Matrix
Tier-1 (low compute): CIFAR-10-C / 100-C, ResNet-18
Tier-2 (mid): ImageNet-C, ResNet-50, ViT-B/16
Tier-3 (stress): Real-world shifts (Wilds, ImageNet-R) & synthetic heavy corruptions severity 5→1 streams
The same protocol (online streaming, batch = 128, natural order) is reused everywhere.

5. Validation Criteria (pass/fail)
A. Speed: B90 reduced by ≥25 % with p<0.05 (paired t-test across seeds)
B. Final accuracy: ∆acc ≥ –0.2 pp relative to best baseline (i.e., not worse)
C. Cost: Runtime overhead ≤1 %, memory overhead ≤50 MB
D. Robustness: std(acc) not higher than baseline; clean-data drop ≤0.2 pp
Meeting A+B+C on Tier-1 plus any two of A/B improvements on Tier-2 signals success. Failing any criterion triggers deeper ablation or hyper-check.

6. Statistical Protocol
• 3 seeds per setting; report mean±std
• Use paired t-test (speed, accuracy) with Holm correction across datasets
• Effect sizes (Cohen’s d) for B90 and ∆acc

7. Reporting Template (all future papers/notes)
Figure 1: Accuracy-vs-batches (all methods)
Table 1: Final acc., B90, B95, AUC, runtime, memory
Table 2: Clean-data drop, std, catastrophic-drift rate
Figure 2: Histogram of adaptive LR factors per batch

8. Computational Logistics
• One 80 GB A100 easily fits batch = 128 for all models; run seeds in parallel via CUDA MPS or sequentially for fairness.
• Always log GPU util and wall-clock with nvprof wrapper.
• Checkpoint BN params every 50 batches for post-hoc trajectory plots (≈1 MB/run).

This strategy supplies a uniform, multi-angle validation framework. Each forthcoming experiment plugs into this template—only the concrete dataset, model and corruption list change—ensuring comparability, statistical rigour and comprehensive evidence for AdaEnt’s effectiveness.

## Current Experiment to Evaluate

**Experiment ID:** exp-1-cifar-core

**Experiment Description:** Objective: Core performance & convergence-speed validation of AdaEnt on low-compute Tier-1 benchmarks (CIFAR-10-C / 100-C) with ResNet-18.

Models:
• Pre-trained ResNet-18 (BN layers unfrozen for adaptation)
• Same backbone for all variations to isolate loss effect.

Datasets:
• CIFAR-10-C, CIFAR-100-C – 15 corruption types × 5 severities.
• Clean CIFAR-10/100 test sets for clean-data drop.

Pre-processing:
• Standard 32×32 centre-crop & per-channel mean/std normalisation.
• No data augmentation during TTA.

Data splitting & streaming:
• Entire corrupted test set is streamed once in natural order with batch_size=128 (online). No access to labels.
• Clean test set is evaluated once before & after adaptation.

Run repetitions:
• 3 random seeds (independent shuffles of stream start index).
• Metrics reported as mean±std; selection = last checkpoint (online scenario has no early stop).

Evaluation metrics:
Primary – Top-1 accuracy, B90, B95, AUC(acc-vs-batches).
Secondary – Expected Calibration Error (ECE, 15 bins), wall-clock per batch, peak GPU mem.

Comparisons / run_variations:
1. frozen – no adaptation (upper-bound on speed, lower-bound on acc.)
2. TENT – baseline.
3. AdaEnt – proposed.
4. AdaEnt-fixed-α – ablation: replace adaptive factor with static α=0.5.
5. oracle-fullbatch-LR – offline TENT with tuned large LR (upper-bound on final acc.).

Hyper-parameter analysis:
• Sweep α∈{0.25,0.5,1.0} for variation 4 (separate grid not part of main run_variations) and LR∈{1e-3,3e-3,1e-2} for oracle; report sensitivity curves.

Robustness checks:
• Noise injection: add iid Gaussian noise σ=0.05 to inputs for last 10% of stream.
• OOD: evaluate on corruption severity 1 after training on severity 5→1.
• Variance across seeds & catastrophic-drift rate.

Computational efficiency:
• Wrap training loop with NVTX, use nvprof to record FLOPs, wall-clock, memory.
• Report overhead relative to TENT.

Example code excerpt (PyTorch):
```python
for x in stream_loader:
    x = x.cuda(non_blocking=True)
    logits = model(x)
    entropy = softmax_entropy(logits).mean(0)
    loss = entropy*entropy/math.log(num_classes)  # AdaEnt
    loss.backward(); optim.step(); optim.zero_grad()
```

Expected outcome: AdaEnt cuts B90 by ≥25 % w.r.t. TENT, keeps final accuracy within ±0.1 pp, runtime overhead <1 %.

**Run Variations:** ['frozen', 'TENT', 'AdaEnt', 'AdaEnt-fixed-α', 'oracle-fullbatch-LR']

**Generated Code:** {"evaluate_py": "", "full_experiment_yaml": "", "main_py": "", "model_py": "", "preprocess_py": "", "pyproject_toml": "", "smoke_test_yaml": "", "train_py": ""}

**Experimental Results:**

- Result: 
- Error: 
- Images: []


**Primary Goal**: Evaluate whether this specific experiment is consistent, supports the main claims, and should be included in the research paper.
Output:
{
    "consistency_feedback": "No scientific consistency can be assessed because the experiment never executed.\n\nIdentified problems\n1. Implementation Issues\n   • All code files are empty (\"\"), nothing was run, so the experimental protocol was not implemented.\n   • Consequently there are no logs, metrics, or figures. Without outputs we cannot verify that AdaEnt’s adaptive-loss was applied, nor that three seeds, corruption sweeps, or ablations were performed.\n   • Effect: zero empirical evidence – the main claims (faster convergence, equal final accuracy, negligible overhead) remain untested.\n\n2. Result Interpretation Issues\n   • Because no numerical results exist, any interpretation would be speculative. The paper cannot cite B90 reductions, accuracy values, or overhead numbers.\n\n3. (Potential) Experimental Strategy Issues\n   • The planned strategy itself is sound and matches the overall play-book (Tier-1 benchmark, correct baselines, required metrics). The only gap is execution, not design.\n\nImpact on paper inclusion\n• The experiment in its present state should NOT be included in the paper. It provides no evidence for or against the proposed method and would weaken the methodological credibility.\n\nRequired to reach minimal publishable consistency\n• Implement and run the experiment exactly as described, record the specified metrics over three seeds, and report mean±std together with statistical tests comparing AdaEnt to TENT. Only then can consistency with the proposed hypotheses be judged.",
    "consistency_score": 2
}
