
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    Improve Test-Time Adaptation in terms of convergence speed.
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
    "Main Contributions": "The paper identifies two key defects in prevalent fully test-time adaptation (FTTA) methods: inaccurate normalization statistics in test-time batch normalization (BN) and biased parameter updates towards dominant classes, especially in complex test environments (dependent or class-imbalanced data). To address these, it proposes DELTA (Degradation-freE fuLly Test-time Adaptation), a plug-in solution comprising Test-time Batch Renormalization (TBR) to improve normalization statistics and Dynamic Online re-weighTing (DOT) to mitigate optimization bias. DELTA consistently improves state-of-the-art FTTA methods across various scenarios, achieving new benchmark results.",
    "Methodology": "The proposed DELTA method consists of two main components: (i) Test-time Batch Renormalization (TBR) improves normalization statistics by using test-time moving averaged statistics (exponential moving averages of mean and standard deviation) to rectify normalized features. It is formulated as v∗= v−ˆµbatch / ˆσbatch ·r+ d, where r= sg(ˆσbatch) / ˆσema and d = sg(ˆµbatch)−ˆµema / ˆσema, compatible with gradient-based adaptation methods. (ii) Dynamic Online re-weighTing (DOT) tackles biased optimization by assigning weights to test samples based on their pseudo-labels and a momentum-updated class-frequency vector. Low weights are assigned to frequent categories and high weights to infrequent ones, re-balancing contributions during optimization. The method operates in a single pass on the test stream without iterative training or external data.",
    "Experimental Setup": "Experiments were conducted on CIFAR100-C, ImageNet-C (using pre-trained ResNeXt-29 and ResNet-50 respectively), and ImageNet-R. A new real-world dataset, YTBB-sub (video segments from YouTube-BoundingBoxes), was introduced with a ResNet-18 model trained on COCO. Four test scenarios were investigated: independently/dependently sampled from class-balanced (IS+CB / DS+CB) and class-imbalanced (IS+CI / DS+CI) distributions. Dependent sampling was simulated using Dirichlet distribution with factor ρ, and class imbalance with exponential decay using factor π. The primary metric was mean accuracy over classes. Baselines included PL, TTA, BN adapt, TENT, MEMO, ETA, Ent-W, LAME, and CoTTA/CoTTA*. DELTA was integrated with PL, TENT, and Ent-W. Ablation studies analyzed individual component contributions, different batch sizes, TBR initialization strategies, and compared DOT with other class-imbalance techniques.",
    "Limitations": "Prevalent test-time adaptation methods suffer from significant performance degradation with small batch sizes, noisy normalization statistics, and biased gradient-based optimization. While DELTA mitigates this, very small batch sizes still pose challenges, making the 'Inherit' initialization strategy for TBR preferable over 'First'. The proposed 'fast-inference and slow-update' strategy for small batches, while effective, involves caching recent test samples, which slightly contradicts strict 'online adaptation' requirements. Other class-imbalance techniques evaluated showed weaknesses, such as sensitivity to thresholds (Div-W), dependence on high-quality class distribution estimates (LA), ineffectiveness in imbalanced scenarios (KL-div), and discarding too many useful samples (Sample-drop). DELTA's performance gains on inherently harder datasets like ImageNet-R were less significant compared to ImageNet-C, suggesting room for improvement on more challenging out-of-distribution cases.",
    "Future Research Directions": "Future research could focus on improving robustness for more extreme out-of-distribution scenarios (e.g., ImageNet-R), developing test-time adaptation methods that are robust to extremely small batch sizes without sacrificing strict online adaptation principles, and exploring dynamic or adaptive strategies for hyperparameter tuning in real-time. Further investigation into more effective online class distribution estimation for techniques like Logit Adjustment, or extending DELTA's framework to address other types of dynamic domain shifts or task variations, could also be fruitful.",
    "Experiment Code": "@torch.jit.scriptdef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\"\"\"Entropy of softmax distribution from logits.\"\"\"return -(x.softmax(1) * x.log_softmax(1)).sum(1)@torch.enable_grad() # ensure grads in possible no grad context for testingdef forward_and_adapt(x, model, optimizer):\"\"\"Forward and adapt model on batch of data.Measure entropy of the model prediction, take gradients, and update params.\"\"\"outputs = model(x)loss = softmax_entropy(outputs).mean(0)loss.backward()optimizer.step()optimizer.zero_grad()return outputsdef configure_model(model):\"\"\"Configure model for use with tent.\"\"\"model.train()model.requires_grad_(False)for m in model.modules():if isinstance(m, nn.BatchNorm2d):m.requires_grad_(True)m.track_running_stats = Falsem.running_mean = Nonem.running_var = Nonereturn modeldef collect_params(model):\"\"\"Collect the affine scale + shift parameters from batch norms.Walk the model's modules and collect all batch normalization parameters.Return the parameters and their names.\"\"\"params = []names = []for nm, m in model.named_modules():if isinstance(m, nn.BatchNorm2d):for np, p in m.named_parameters():if np in ['weight', 'bias']:params.append(p)names.append(f\"{nm}.{np}\")return params, namesclass Tent(nn.Module):\"\"\"Tent adapts a model by entropy minimization during testing.\"\"\"def __init__(self, model, optimizer, steps=1, episodic=False):super().__init__()self.model = modelself.optimizer = optimizerself.steps = stepsassert steps > 0, \"tent requires >= 1 step(s) to forward and update\"self.episodic = episodicself.model_state, self.optimizer_state = copy_model_and_optimizer(self.model, self.optimizer)def forward(self, x):if self.episodic:self.reset()for _ in range(self.steps):outputs = forward_and_adapt(x, self.model, self.optimizer)return outputsdef reset(self):if self.model_state is None or self.optimizer_state is None:raise Exception(\"cannot reset without saved model/optimizer state\")load_model_and_optimizer(self.model, self.optimizer,self.model_state, self.optimizer_state)",
    "Experiment Result": "The repository content implements Test-time Entropy Minimization (TENT), which is used as the adaptation method. For the Test-time Batch Renormalization (TBR) aspect of the DELTA method, the TENT implementation's `configure_model` function sets `nn.BatchNorm2d` layers to `model.train()` and disables tracking of running statistics (`m.track_running_stats = False`, `m.running_mean = None`, `m.running_var = None`). This forces BatchNorm layers to use batch-wise mean and variance for normalization during adaptation. However, the specific rectification formula involving exponential moving averaged statistics (`ˆµema`, `ˆσema`) as described in DELTA's TBR is not explicitly present. The Dynamic Online re-weighTing (DOT) component, which involves assigning weights to test samples based on pseudo-labels and a momentum-updated class-frequency vector, is not implemented; the loss calculation `softmax_entropy(outputs).mean(0)` applies an unweighted mean. Experimental settings include: 1 update step per batch (`OPTIM.STEPS = 1`), learning rate `OPTIM.LR = 1e-3`, Adam optimizer (`OPTIM.METHOD = 'Adam'`, `OPTIM.BETA = 0.9`) or SGD (`OPTIM.METHOD = 'SGD'`, `OPTIM.MOMENTUM = 0.9`, `OPTIM.NESTEROV = True`), weight decay `OPTIM.WD = 0.0`. The batch size for evaluation and updates is `TEST.BATCH_SIZE = 128`. Adaptation is non-episodic by default (`MODEL.EPISODIC = False`). Evaluation is performed on CIFAR-10-C with various corruption types (e.g., 'gaussian_noise', 'defocus_blur', 'brightness') at severities `[5, 4, 3, 2, 1]`."
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "experimental_setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "experimental_code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "expected_result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "expected_conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}
