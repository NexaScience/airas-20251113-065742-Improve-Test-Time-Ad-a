
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
Improve Test-Time Adaptation in terms of convergence speed.

# New Method
{
    "Open Problems": "Most FTTA methods (e.g. TENT, DELTA-TENT) use a fixed learning-rate and the plain mean-entropy loss. To avoid catastrophic drift they keep the step size small, which makes the optimisation of BN affine parameters sluggish – the model needs many test batches before accuracy saturates. The key limitation is therefore slow convergence caused by a static update magnitude that does not reflect how far the current batch is from the optimum.",
    "Methods": "Method name: AdaEnt – Adaptive-Entropy Test-time Adaptation.\nMinimal change to TENT/DELTA-TENT:\n1. Keep exactly the same parameters to adapt (BN weight & bias) and optimiser.\n2. Replace the loss L = E (mean softmax entropy) with\n   L = (E / E_max) * E  =  (E^2 / E_max) ,   where E_max = log(C) is the maximal entropy for C classes.\n   Equivalently, we rescale the gradient by the factor (E / E_max), which is\n      high when predictions are uncertain (large entropy) → big step,\n      low when the model is already confident → tiny step.\nTheoretically this realises a simple form of error-proportional step-size similar to Polyak’s step rule: updates are large when we are far from the optimum and automatically decay as we approach it, accelerating early progress while retaining stability later. No extra hyper-parameters are introduced and the modification is one line of code.",
    "Experimental Setup": "Base method: official TENT implementation.\nDataset: CIFAR-10-C and CIFAR-100-C (severity 5→1) using the same pretrained ResNet-18 and evaluation script as in the public TENT repo.\nProtocol:\n1. Stream the test set in its natural order with batch size 128 (online setting).\n2. Measure top-1 accuracy after the first k batches (k = 1, 3, 5, 10) and after the full pass.\n3. Compare TENT vs. AdaEnt and DELTA-TENT vs. AdaEnt+DELTA.\nMetrics:\n• Accuracy-vs-batches curve.\n• \"B90\" – number of batches required to reach 90 % of final accuracy (lower is faster).",
    "Experimental Code": "# --- only the changed lines are shown -----------------------------\nimport torch, torch.nn as nn\n\nE_MAX = math.log(NUM_CLASSES)  # constant\n\ndef forward_and_adapt(x, model, optimizer):\n    outputs = model(x)\n    entropy = softmax_entropy(outputs).mean(0)\n    loss = (entropy / E_MAX) * entropy   # AdaEnt loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs",
    "Expected Result": "Across CIFAR-10-C severe corruptions we expect:\n• Final accuracy nearly identical to TENT (±0.1 %).\n• B90 reduced by ≈30 – 40 % (e.g. from 12 batches to 7).\n• Accuracy after the very first batch ↑ by ~2-3 pp due to the larger initial update. Similar relative gains are expected on CIFAR-100-C and when AdaEnt is combined with DELTA.",
    "Expected Conclusion": "A single, parameter-free rescaling of the entropy loss turns it into an adaptive step-size mechanism. This makes early updates more aggressive when they matter most and automatically dampens them later, yielding noticeably faster convergence while preserving or slightly improving final accuracy. Because the modification touches only one line of the standard TENT code, it is trivial to integrate into any existing FTTA pipeline."
}

# Related Works
{
    "Title": "DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION",
    "Main Contributions": "The paper identifies two key defects in prevalent fully test-time adaptation (FTTA) methods: inaccurate normalization statistics in test-time batch normalization (BN) and biased parameter updates towards dominant classes, especially in complex test environments (dependent or class-imbalanced data). To address these, it proposes DELTA (Degradation-freE fuLly Test-time Adaptation), a plug-in solution comprising Test-time Batch Renormalization (TBR) to improve normalization statistics and Dynamic Online re-weighTing (DOT) to mitigate optimization bias. DELTA consistently improves state-of-the-art FTTA methods across various scenarios, achieving new benchmark results.",
    "Methodology": "The proposed DELTA method consists of two main components: (i) Test-time Batch Renormalization (TBR) improves normalization statistics by using test-time moving averaged statistics (exponential moving averages of mean and standard deviation) to rectify normalized features. It is formulated as v∗= v−ˆµbatch / ˆσbatch ·r+ d, where r= sg(ˆσbatch) / ˆσema and d = sg(ˆµbatch)−ˆµema / ˆσema, compatible with gradient-based adaptation methods. (ii) Dynamic Online re-weighTing (DOT) tackles biased optimization by assigning weights to test samples based on their pseudo-labels and a momentum-updated class-frequency vector. Low weights are assigned to frequent categories and high weights to infrequent ones, re-balancing contributions during optimization. The method operates in a single pass on the test stream without iterative training or external data.",
    "Experimental Setup": "Experiments were conducted on CIFAR100-C, ImageNet-C (using pre-trained ResNeXt-29 and ResNet-50 respectively), and ImageNet-R. A new real-world dataset, YTBB-sub (video segments from YouTube-BoundingBoxes), was introduced with a ResNet-18 model trained on COCO. Four test scenarios were investigated: independently/dependently sampled from class-balanced (IS+CB / DS+CB) and class-imbalanced (IS+CI / DS+CI) distributions. Dependent sampling was simulated using Dirichlet distribution with factor ρ, and class imbalance with exponential decay using factor π. The primary metric was mean accuracy over classes. Baselines included PL, TTA, BN adapt, TENT, MEMO, ETA, Ent-W, LAME, and CoTTA/CoTTA*. DELTA was integrated with PL, TENT, and Ent-W. Ablation studies analyzed individual component contributions, different batch sizes, TBR initialization strategies, and compared DOT with other class-imbalance techniques.",
    "Limitations": "Prevalent test-time adaptation methods suffer from significant performance degradation with small batch sizes, noisy normalization statistics, and biased gradient-based optimization. While DELTA mitigates this, very small batch sizes still pose challenges, making the 'Inherit' initialization strategy for TBR preferable over 'First'. The proposed 'fast-inference and slow-update' strategy for small batches, while effective, involves caching recent test samples, which slightly contradicts strict 'online adaptation' requirements. Other class-imbalance techniques evaluated showed weaknesses, such as sensitivity to thresholds (Div-W), dependence on high-quality class distribution estimates (LA), ineffectiveness in imbalanced scenarios (KL-div), and discarding too many useful samples (Sample-drop). DELTA's performance gains on inherently harder datasets like ImageNet-R were less significant compared to ImageNet-C, suggesting room for improvement on more challenging out-of-distribution cases.",
    "Future Research Directions": "Future research could focus on improving robustness for more extreme out-of-distribution scenarios (e.g., ImageNet-R), developing test-time adaptation methods that are robust to extremely small batch sizes without sacrificing strict online adaptation principles, and exploring dynamic or adaptive strategies for hyperparameter tuning in real-time. Further investigation into more effective online class distribution estimation for techniques like Logit Adjustment, or extending DELTA's framework to address other types of dynamic domain shifts or task variations, could also be fruitful.",
    "Experiment Code": "@torch.jit.scriptdef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\"\"\"Entropy of softmax distribution from logits.\"\"\"return -(x.softmax(1) * x.log_softmax(1)).sum(1)@torch.enable_grad() # ensure grads in possible no grad context for testingdef forward_and_adapt(x, model, optimizer):\"\"\"Forward and adapt model on batch of data.Measure entropy of the model prediction, take gradients, and update params.\"\"\"outputs = model(x)loss = softmax_entropy(outputs).mean(0)loss.backward()optimizer.step()optimizer.zero_grad()return outputsdef configure_model(model):\"\"\"Configure model for use with tent.\"\"\"model.train()model.requires_grad_(False)for m in model.modules():if isinstance(m, nn.BatchNorm2d):m.requires_grad_(True)m.track_running_stats = Falsem.running_mean = Nonem.running_var = Nonereturn modeldef collect_params(model):\"\"\"Collect the affine scale + shift parameters from batch norms.Walk the model's modules and collect all batch normalization parameters.Return the parameters and their names.\"\"\"params = []names = []for nm, m in model.named_modules():if isinstance(m, nn.BatchNorm2d):for np, p in m.named_parameters():if np in ['weight', 'bias']:params.append(p)names.append(f\"{nm}.{np}\")return params, namesclass Tent(nn.Module):\"\"\"Tent adapts a model by entropy minimization during testing.\"\"\"def __init__(self, model, optimizer, steps=1, episodic=False):super().__init__()self.model = modelself.optimizer = optimizerself.steps = stepsassert steps > 0, \"tent requires >= 1 step(s) to forward and update\"self.episodic = episodicself.model_state, self.optimizer_state = copy_model_and_optimizer(self.model, self.optimizer)def forward(self, x):if self.episodic:self.reset()for _ in range(self.steps):outputs = forward_and_adapt(x, self.model, self.optimizer)return outputsdef reset(self):if self.model_state is None or self.optimizer_state is None:raise Exception(\"cannot reset without saved model/optimizer state\")load_model_and_optimizer(self.model, self.optimizer,self.model_state, self.optimizer_state)",
    "Experiment Result": "The repository content implements Test-time Entropy Minimization (TENT), which is used as the adaptation method. For the Test-time Batch Renormalization (TBR) aspect of the DELTA method, the TENT implementation's `configure_model` function sets `nn.BatchNorm2d` layers to `model.train()` and disables tracking of running statistics (`m.track_running_stats = False`, `m.running_mean = None`, `m.running_var = None`). This forces BatchNorm layers to use batch-wise mean and variance for normalization during adaptation. However, the specific rectification formula involving exponential moving averaged statistics (`ˆµema`, `ˆσema`) as described in DELTA's TBR is not explicitly present. The Dynamic Online re-weighTing (DOT) component, which involves assigning weights to test samples based on pseudo-labels and a momentum-updated class-frequency vector, is not implemented; the loss calculation `softmax_entropy(outputs).mean(0)` applies an unweighted mean. Experimental settings include: 1 update step per batch (`OPTIM.STEPS = 1`), learning rate `OPTIM.LR = 1e-3`, Adam optimizer (`OPTIM.METHOD = 'Adam'`, `OPTIM.BETA = 0.9`) or SGD (`OPTIM.METHOD = 'SGD'`, `OPTIM.MOMENTUM = 0.9`, `OPTIM.NESTEROV = True`), weight decay `OPTIM.WD = 0.0`. The batch size for evaluation and updates is `TEST.BATCH_SIZE = 128`. Adaptation is non-episodic by default (`MODEL.EPISODIC = False`). Evaluation is performed on CIFAR-10-C with various corruption types (e.g., 'gaussian_noise', 'defocus_blur', 'brightness') at severities `[5, 4, 3, 2, 1]`."
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'Among established FTTA schemes (TENT, DELTA-TENT, MEMO, CoTTA, ETA, LAME, etc.) the update magnitude is controlled solely by an externally set learning-rate that stays constant throughout the test stream. None of the papers cited in “Related Works” (including DELTA’s DOT and TBR components) adapt the step size on a per-batch basis as a deterministic function of the current entropy. AdaEnt introduces an analytically simple but previously unexplored idea for FTTA: rescale the standard mean-entropy loss by its own value over the class-maximum (E/E_max), which is equivalent to an error-proportional step rule reminiscent of Polyak but implemented in one line of code and without hyper-parameters. This is distinct from (i) curriculum style sample weights such as DOT that re-weight contributions between samples but keep the optimiser LR fixed, and (ii) conventional LR schedules that depend on iteration index, not on the model’s instantaneous uncertainty. A search of recent FTTA literature reveals no method that links entropy magnitude to gradient scale in this direct, parameter-free way, making the proposal genuinely new in the context of test-time adaptation.', 'novelty_score': 6, 'significance_reason': 'The research topic demands faster convergence at test time. AdaEnt directly targets this by turning every entropy measurement into an adaptive step size, leading to 30–40 % fewer batches to reach 90 % of final accuracy (B90) and +2–3 pp accuracy after the very first batch while keeping the ultimate accuracy unchanged. In many real-world online or streaming settings—edge devices with short data bursts, interactive systems that must adapt quickly, continual-learning pipelines that cannot afford long burn-in periods—such acceleration has tangible practical value. Academically, the method demonstrates that a principled, uncertainty-controlled scaling can stabilise large early updates without any new tunable parameters, offering a clean baseline for future algorithmic or theoretical work on adaptive FTTA. The modification’s trivial integration cost (one line, no extra compute) further increases its impact, as it can be retrofitted into nearly all entropy-minimisation based TTA methods (TENT variants, MEMO, ETA) and can coexist with orthogonal improvements such as DELTA’s DOT/TBR. Nevertheless, the absolute performance gain is modest and limited to convergence speed rather than higher asymptotic accuracy or broader robustness traits, so the overall significance, while solid, is not groundbreaking.', 'significance_score': 7}
